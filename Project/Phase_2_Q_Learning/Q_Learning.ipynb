{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Importing Relevant Libraries and Python Scripts"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7269,"status":"ok","timestamp":1699075671079,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"V_gyn2zfuQqe","outputId":"21e841a0-a420-4405-a8c2-d65f866a6c86"},"outputs":[],"source":["import numpy as np\n","import os\n","import pickle\n","\n","# Setting Directory\n","os.chdir('C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/reinforcement_learning/Project/')\n","\n","from python_scripts import state_formulation, utils, algorithm"]},{"cell_type":"markdown","metadata":{},"source":["### Q - Learning + Testing"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"JUjy79lLC43B"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Iteration: 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db27925885d34d31808a48bf60d065af","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["delta: 0.25215432553062955, epsilon: 1.0\n","Starting Iteration: 2\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e008c1e7e3c44709049ecdc9a62c0ef","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["delta: 0.061000481118874184, epsilon: 0.0078125\n","Starting Iteration: 3\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e893d78df8bc4d84bb2fff40b439758b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["delta: 0.04769873066965724, epsilon: 0.0078125\n","Starting Iteration: 4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f6797caff7b49b495aa60f8ac79d541","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["delta: 0.0077168456239899985, epsilon: 0.0078125\n","Starting Iteration: 5\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8b142c44e4b41f7932b931e49ae8d80","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["delta: 0.2606093426667537, epsilon: 0.0078125\n","Starting Iteration: 6\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"efe9dc46dc794ad484cf5f5652651044","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["delta: 0.11952548278376651, epsilon: 0.0078125\n","Starting Iteration: 7\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92f0bb4878654e98a21e3d1c16ed0448","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["delta: 0.05017709945033655, epsilon: 0.0078125\n","Starting Iteration: 8\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"616589f43a0e417e9343d755862dd946","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["delta: 0.021595916719856056, epsilon: 0.0078125\n","Starting Iteration: 9\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97e05dc1584244d7a4049886f1dc6955","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["delta: 0.24414048733813143, epsilon: 0.0078125\n","Starting Iteration: 10\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6de97c78c1d44158458b2b7381c82ca","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["delta: 0.02324316931363346, epsilon: 0.0078125\n"]}],"source":["# Defining parameters for value iteration and getting the states\n","\n","map_size = 4\n","gamma = 0.9\n","print(f'Dataset Generation Started....')\n","state_space = state_formulation.prune_and_get_total_states(grid_size = map_size) # 10165779 length\n","print(f'Dataset Generated....')\n","update_count_table = np.zeros(((3 ** (map_size ** 2)), map_size ** 2))\n","count_table = np.zeros((3 ** (map_size ** 2)))\n","q_table = np.random.normal(size = ((3 ** (map_size ** 2)), map_size ** 2))\n","max_policy = np.zeros((3 ** (map_size ** 2)), dtype = int)\n","min_policy = np.zeros((3 ** (map_size ** 2)), dtype = int)\n","epsilon = 1\n","lr = 0.1\n","adap_lr = 0.01\n","thres = 1e-10\n","\n","# Running Q-Learning with 10 episodes until convergence\n","total_reward = 0\n","for i in range(10):\n","    print(f'Starting Iteration: {i + 1}')\n","    delta, q_table, count_table, update_count_table, total_reward, epsilon = \\\n","        algorithm.q_learning(map_size, epsilon, gamma, lr, adap_lr, total_reward, \n","                             state_space, q_table, update_count_table, count_table, (i + 1))\n","    print(f'delta: {delta}, epsilon: {epsilon}')\n","    if delta < thres: \n","        print(f'Q - Learning for Tic-Tac-Toe Game Converged at iteration: {i + 1}')\n","        break\n","\n","# Once value iteration has converged, use the q-table and argmax per row to get optimal policy for each state\n","\n","for s, state in enumerate(state_space):\n","    if state_formulation.ongoing_state(map_size, state):\n","        actions = utils.get_actions(state)\n","        player = utils.get_player(state)\n","        if player == 1: max_policy[utils.get_ternanry_conversion(state)] = \\\n","        actions[np.argmax(q_table[utils.get_ternanry_conversion(state), actions])]\n","        if player == 2: min_policy[utils.get_ternanry_conversion(state)] = \\\n","        actions[np.argmin(q_table[utils.get_ternanry_conversion(state), actions])]"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0|0|0\n","-----\n","0|0|0\n","-----\n","0|0|0\n","\n","1\n","1|0|0\n","-----\n","0|0|0\n","-----\n","0|0|0\n","\n"]},{"ename":"IndexError","evalue":"list index out of range","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Talha\\OneDrive - Higher Education Commission\\Documents\\GitHub\\reinforcement_learning\\Project\\Phase_2_Q_Learning\\Q_Learning.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive%20-%20Higher%20Education%20Commission/Documents/GitHub/reinforcement_learning/Project/Phase_2_Q_Learning/Q_Learning.ipynb#W4sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     board, player, terminated \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(max_policy[\u001b[39mint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m env\u001b[39m.\u001b[39mboard]), base \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive%20-%20Higher%20Education%20Commission/Documents/GitHub/reinforcement_learning/Project/Phase_2_Q_Learning/Q_Learning.ipynb#W4sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mif\u001b[39;00m player \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive%20-%20Higher%20Education%20Commission/Documents/GitHub/reinforcement_learning/Project/Phase_2_Q_Learning/Q_Learning.ipynb#W4sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     board, player, terminated \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(min_policy[\u001b[39mint\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin([\u001b[39mstr\u001b[39;49m(i) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m env\u001b[39m.\u001b[39;49mboard]), base \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive%20-%20Higher%20Education%20Commission/Documents/GitHub/reinforcement_learning/Project/Phase_2_Q_Learning/Q_Learning.ipynb#W4sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mif\u001b[39;00m terminated:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive%20-%20Higher%20Education%20Commission/Documents/GitHub/reinforcement_learning/Project/Phase_2_Q_Learning/Q_Learning.ipynb#W4sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     env\u001b[39m.\u001b[39mprint_board()\n","\u001b[1;32mc:\\Users\\Talha\\OneDrive - Higher Education Commission\\Documents\\GitHub\\reinforcement_learning\\Project\\Phase_2_Q_Learning\\Q_Learning.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive%20-%20Higher%20Education%20Commission/Documents/GitHub/reinforcement_learning/Project/Phase_2_Q_Learning/Q_Learning.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, position):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive%20-%20Higher%20Education%20Commission/Documents/GitHub/reinforcement_learning/Project/Phase_2_Q_Learning/Q_Learning.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboard[position] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive%20-%20Higher%20Education%20Commission/Documents/GitHub/reinforcement_learning/Project/Phase_2_Q_Learning/Q_Learning.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard[position] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_player\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive%20-%20Higher%20Education%20Commission/Documents/GitHub/reinforcement_learning/Project/Phase_2_Q_Learning/Q_Learning.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_win(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_player):\n","\u001b[1;31mIndexError\u001b[0m: list index out of range"]}],"source":["# Class for Testing Policy - already provided us the template\n","class TicTacToe:\n","    def __init__(self):\n","        self.board = [0 for _ in range(9)]\n","        self.current_player = 1\n","\n","    def print_board(self):\n","        for i in range(0, 9, 3):\n","            print(str(self.board[i]) + \"|\" + str(self.board[i + 1]) + \"|\" + str(self.board[i + 2]))\n","            if i < 6:\n","                print(\"-\" * 5)\n","        print()\n","\n","    def check_win(self, player):\n","        print(player)\n","        win_conditions = [(0, 1, 2), (3, 4, 5), (6, 7, 8),\n","                          (0, 3, 6), (1, 4, 7), (2, 5, 8),\n","                          (0, 4, 8), (2, 4, 6)]\n","\n","        for condition in win_conditions:\n","            if all(self.board[i] == player for i in condition):\n","                return True\n","        return False\n","\n","    def step(self, position):\n","        if self.board[position] == 0:\n","            self.board[position] = self.current_player\n","            if self.check_win(self.current_player):\n","                return self.board, self.current_player, True\n","            elif 0 not in self.board:\n","                return self.board, 0, True\n","            self.current_player = 2 if self.current_player == 1 else 1\n","            return self.board, self.current_player, False\n","        else:\n","            print(\"Cell already occupied. Try again.\")\n","            return self.board, self.current_player, False\n","\n","    def reset(self):\n","        self.__init__()\n","\n","env = TicTacToe()\n","\n","# Simulating TicTacToe Game. Note that the optimal policy should always be a draw.\n","while True:\n","    env.print_board() # You can comment this part out if you don't want to see the board\n","\n","    if player == 1:\n","        board, player, terminated = env.step(max_policy[int(''.join([str(i) for i in env.board]), base = 3)])\n","    if player == 2:\n","        board, player, terminated = env.step(min_policy[int(''.join([str(i) for i in env.board]), base = 3)])\n","\n","    if terminated:\n","        env.print_board()\n","        print(\"Player 1 wins\") if player == 1 else print(\"Player 2 wins\") if player == 2 else print(\"It's a draw\")\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPOajkyiUp8yaF54WGz5ADk","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
