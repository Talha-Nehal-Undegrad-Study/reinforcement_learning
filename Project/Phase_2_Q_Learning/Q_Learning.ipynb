{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Importing Relevant Libraries and Python Scripts"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7269,"status":"ok","timestamp":1699075671079,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"V_gyn2zfuQqe","outputId":"21e841a0-a420-4405-a8c2-d65f866a6c86"},"outputs":[],"source":["import numpy as np\n","import os\n","import pickle\n","\n","# Setting Directory\n","os.chdir('C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/reinforcement_learning/Project/')\n","\n","from python_scripts import state_formulation, utils, algorithm"]},{"cell_type":"markdown","metadata":{},"source":["### Value Iteration + Testing"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"JUjy79lLC43B"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Iteration: 1\n","Value Iteration for Tic-Tac-Toe Game Converged at iteration: 1\n","0|0|0\n","-----\n","0|0|0\n","-----\n","0|0|0\n","\n","1\n","2\n","1|2|0\n","-----\n","0|0|0\n","-----\n","0|0|0\n","\n","1\n","2\n","1|2|1\n","-----\n","2|0|0\n","-----\n","0|0|0\n","\n","1\n","2\n","1|2|1\n","-----\n","2|1|2\n","-----\n","0|0|0\n","\n","1\n","1|2|1\n","-----\n","2|1|2\n","-----\n","1|0|0\n","\n","Player 1 wins\n"]}],"source":["# Defining parameters for value iteration and getting the states\n","\n","map_size = 3\n","gamma = 0.9\n","state_space = state_formulation.prune_and_get_total_states(grid_size = map_size) # 10165779 length\n","update_count_table = np.zeros(((3 ** (map_size ** 2)), map_size ** 2))\n","count_table = np.zeros((3 ** (map_size ** 2)))\n","q_table = np.zeros(((3 ** (map_size ** 2)), map_size ** 2))\n","max_policy = np.zeros((3 ** (map_size ** 2)), dtype = int)\n","min_policy = np.zeros((3 ** (map_size ** 2)), dtype = int)\n","epsilon = 0.9\n","lr = 0.1\n","adap_lr = 0.01\n","thres = 1e-10\n","\n","# Running value iteration until convergence.\n","total_reward = 0\n","for i in range(10000):\n","    print(f'Starting Iteration: {i + 1}')\n","    delta, q_table, count_table, update_count_table, total_reward = \\\n","        algorithm.q_learning(map_size, epsilon, gamma, lr, adap_lr, total_reward, state_space, q_table, update_count_table, count_table)\n","    \n","    if delta < 1e-10: \n","        print(f'Value Iteration for Tic-Tac-Toe Game Converged at iteration: {i + 1}')\n","        break\n","\n","# Once value iteration has converged, use the q-table and argmax per row to get optimal policy for each state\n","\n","for s, state in enumerate(state_space):\n","    if state_formulation.ongoing_state(map_size, state):\n","        actions = utils.get_actions(state)\n","        player = utils.get_player(state)\n","        if player == 1: max_policy[utils.get_ternanry_conversion(state)] = \\\n","        actions[np.argmax(q_table[utils.get_ternanry_conversion(state), actions])]\n","        if player == 2: min_policy[utils.get_ternanry_conversion(state)] = \\\n","        actions[np.argmin(q_table[utils.get_ternanry_conversion(state), actions])]\n","\n","# Class for Testing Policy - already provided us the template\n","class TicTacToe:\n","    def __init__(self):\n","        self.board = [0 for _ in range(9)]\n","        self.current_player = 1\n","\n","    def print_board(self):\n","        for i in range(0, 9, 3):\n","            print(str(self.board[i]) + \"|\" + str(self.board[i + 1]) + \"|\" + str(self.board[i + 2]))\n","            if i < 6:\n","                print(\"-\" * 5)\n","        print()\n","\n","    def check_win(self, player):\n","        print(player)\n","        win_conditions = [(0, 1, 2), (3, 4, 5), (6, 7, 8),\n","                          (0, 3, 6), (1, 4, 7), (2, 5, 8),\n","                          (0, 4, 8), (2, 4, 6)]\n","\n","        for condition in win_conditions:\n","            if all(self.board[i] == player for i in condition):\n","                return True\n","        return False\n","\n","    def step(self, position):\n","        if self.board[position] == 0:\n","            self.board[position] = self.current_player\n","            if self.check_win(self.current_player):\n","                return self.board, self.current_player, True\n","            elif 0 not in self.board:\n","                return self.board, 0, True\n","            self.current_player = 2 if self.current_player == 1 else 1\n","            return self.board, self.current_player, False\n","        else:\n","            print(\"Cell already occupied. Try again.\")\n","            return self.board, self.current_player, False\n","\n","    def reset(self):\n","        self.__init__()\n","\n","env = TicTacToe()\n","\n","# Simulating TicTacToe Game. Note that the optimal policy should always be a draw.\n","while True:\n","    env.print_board() # You can comment this part out if you don't want to see the board\n","\n","    if player == 1:\n","        board, player, terminated = env.step(max_policy[int(''.join([str(i) for i in env.board]), base = 3)])\n","    if player == 2:\n","        board, player, terminated = env.step(min_policy[int(''.join([str(i) for i in env.board]), base = 3)])\n","\n","    if terminated:\n","        env.print_board()\n","        print(\"Player 1 wins\") if player == 1 else print(\"Player 2 wins\") if player == 2 else print(\"It's a draw\")\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPOajkyiUp8yaF54WGz5ADk","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
