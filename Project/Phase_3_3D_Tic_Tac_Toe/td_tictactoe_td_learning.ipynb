{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e9351a0",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning for 3D Tic Tac Toe\n",
    "\n",
    "This notebook contains the implementation of a Temporal Difference (TD) learning model using a Deep Q-Network (DQN) for playing 3D 4x4x4 Tic Tac Toe. The implementation is based on the approach outlined in the provided paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261768de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Setting Directory\n",
    "os.chdir('C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/reinforcement_learning/Project/')\n",
    "\n",
    "from python_scripts import state_formulation, utils, algorithm, tictactoe_4x4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "import torch.nn.init as init\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97cae957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class customDotProduct(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, block_size = 4):\n",
    "#         super(customDotProduct, self).__init__()\n",
    "#         self.input_size = input_size\n",
    "#         self.output_size = output_size\n",
    "#         self.block_size = block_size\n",
    "#         # Convert structure_weight to nn.Parameter\n",
    "#         self.structure_weight = torch.zeros((self.output_size, self.input_size))\n",
    "#         self.structure_weight = self.get_block_weights(self.structure_weight, block_size)\n",
    "#         self.structure_weight = nn.ParameterList([nn.Parameter(sw.float()) for sw in self.structure_weight])\n",
    "\n",
    "#     def get_block_weights(self, weight_list, block_size):\n",
    "#         for i in range(0, 304, block_size):\n",
    "#             weight_list[i: i + block_size, i: i + block_size] = init.xavier_normal_(torch.randn(block_size, block_size))\n",
    "#         learnable_blocks = [weight_list[i:i + block_size, i:i + block_size] for i in range(0, weight_list.shape[0], block_size)]\n",
    "#         updated = [block for block in learnable_blocks]\n",
    "#         return updated\n",
    "    \n",
    "#     def forward(self, feature_map):\n",
    "#         self.feature_map = [fm.float() for fm in feature_map]\n",
    "#         # Calculate dot products and concatenate along dim = 1\n",
    "#         concatenated_products = torch.cat([torch.matmul(fm.unsqueeze(0), sw) for fm, sw in zip(self.feature_map, self.structure_weight)], dim = 1)\n",
    "#         return concatenated_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8005ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing Code\n",
    "# weights = torch.zeros((304, 304))\n",
    "\n",
    "# block_size = 4\n",
    "# for i in range(0, 304, block_size):\n",
    "#     weights[i: i + block_size, i: i + block_size] = torch.ones(block_size, block_size)\n",
    "# learnable_blocks = [weights[i:i + block_size, i:i + block_size] for i in range(0, weights.shape[0], block_size)]\n",
    "# weights = [init.xavier_normal_(block) for block in learnable_blocks]\n",
    "\n",
    "# print(f'Before Update: Weights = {weights[0]} \\n')\n",
    "\n",
    "# # Assume some loss function and optimizer have been defined\n",
    "# custom_dot_product_module = customDotProduct(304, 304)\n",
    "# loss_function = nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(custom_dot_product_module.parameters(), lr = 0.01)\n",
    "\n",
    "# # Example training loop iteration\n",
    "# optimizer.zero_grad()  # Clear gradients\n",
    "# output = custom_dot_product_module(rows)  # Perform forward pass\n",
    "# loss = loss_function(output, torch.randn(1, 304))  # Compute loss\n",
    "# loss.backward()  # Perform backward pass\n",
    "# optimizer.step()  # Update weights\n",
    "# print(f'After Update: Weights = {custom_dot_product_module.structure_weight[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba55757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StructuredLinear(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(StructuredLinear, self).__init__()\n",
    "\n",
    "#     def get_rows(self, input_tensor):\n",
    "#         # Get diagonals (across 2 faces),digonals (across 3 faces) and horizontal and vertical rows\n",
    "#         diag_two_faces = []\n",
    "#         diag_two_faces.extend(\n",
    "#             [torch.diagonal(input_tensor[i, :, :]), torch.diagonal(input_tensor[:, i, :]), torch.diagonal(input_tensor[:, :, i]), \n",
    "#             torch.diagonal(torch.fliplr(input_tensor)[i, :, :]), torch.diagonal(torch.fliplr(input_tensor)[:, i, :]), torch.diagonal(torch.fliplr(input_tensor)[:, :, i])] \n",
    "#             for i in range(input_tensor.shape[0]))\n",
    "#         diag_two_faces = [item for sublist in diag_two_faces for item in sublist]\n",
    "        \n",
    "#         diag_three_faces = []\n",
    "#         diag_three_faces = [[[[input_tensor[i, i, i], input_tensor[3 - i, i, i], input_tensor[i, 3 - i, i], input_tensor[i, i, 3 - i]] \n",
    "#                             for i in range(4)][k][j] for j in range(4) for k in range(4)][l:l + 4] for l in range(0, 16, 4)]\n",
    "#         diag_three_faces = [torch.tensor([t.item() for t in row]) for row in diag_three_faces]\n",
    "\n",
    "#         horizontal_and_vertical_rows = []\n",
    "#         horizontal_and_vertical_rows.extend([input_tensor[i, j, :], input_tensor[i, :, j], input_tensor[:, i, j]]\n",
    "#                                             for i in range(input_tensor.shape[0]) for j in range(input_tensor.shape[0]))\n",
    "#         horizontal_and_vertical_rows = [item for sublist in horizontal_and_vertical_rows for item in sublist]\n",
    "        \n",
    "#         return horizontal_and_vertical_rows + diag_two_faces + diag_three_faces\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         rows = self.get_rows(x)\n",
    "#         return rows\n",
    "\n",
    "# class MyNeuralNetwork(nn.Module, tictactoe_4x4.TicTacToe4x4x4):\n",
    "#     def __init__(self, num_detectors):\n",
    "#         super(MyNeuralNetwork, self).__init__()\n",
    "\n",
    "#         self.num_detectors = num_detectors\n",
    "#         self.structured_layer = StructuredLinear()\n",
    "        \n",
    "#         self.custom_operation_layer = customDotProduct(input_size = num_detectors * 4, output_size = num_detectors * 4)\n",
    "\n",
    "#         self.second_layer = nn.Linear(num_detectors * 4, 32, bias = False)\n",
    "#         init.xavier_normal_(self.second_layer.weight)\n",
    "\n",
    "#         self.output_layer = nn.Linear(32, 1, bias = False)\n",
    "#         self.act = nn.Tanh()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.structured_layer(x)\n",
    "#         x = self.custom_operation_layer(x)\n",
    "#         x = self.act(x) # --> Tanh\n",
    "#         x = self.second_layer(x)\n",
    "#         x = self.act(x)\n",
    "#         x = self.output_layer(x)\n",
    "#         return x\n",
    "    \n",
    "# # Instantiate Model\n",
    "# model = MyNeuralNetwork(num_detectors = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a36f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDotProduct(nn.Module):\n",
    "    def __init__(self, input_size, output_size, block_size = 4):\n",
    "        super(customDotProduct, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.block_size = block_size\n",
    "        # Convert structure_weight to nn.Parameter\n",
    "        self.structure_weight = torch.zeros((self.output_size, self.input_size))\n",
    "        self.structure_weight = self.get_block_weights(self.structure_weight, block_size)\n",
    "        self.structure_weight = nn.ParameterList([nn.Parameter(sw.float()) for sw in self.structure_weight])\n",
    "\n",
    "    def get_block_weights(self, weight_list, block_size):\n",
    "        for i in range(0, 304, block_size):\n",
    "            weight_list[i: i + block_size, i: i + block_size] = init.xavier_normal_(torch.randn(block_size, block_size))\n",
    "        learnable_blocks = [weight_list[i:i + block_size, i:i + block_size] for i in range(0, weight_list.shape[0], block_size)]\n",
    "        updated = [block for block in learnable_blocks]\n",
    "        return updated\n",
    "    \n",
    "    def forward(self, feature_map):\n",
    "        self.feature_map = [fm.float() for fm in feature_map]\n",
    "        # Calculate dot products and concatenate along dim = 1\n",
    "        concatenated_products = torch.cat([torch.matmul(fm.unsqueeze(0), sw) for fm, sw in zip(self.feature_map, self.structure_weight)], dim = 1)\n",
    "        return concatenated_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09ad6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuredLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StructuredLinear, self).__init__()\n",
    "\n",
    "    def get_rows(self, input_tensor):\n",
    "        # Get diagonals (across 2 faces),digonals (across 3 faces) and horizontal and vertical rows\n",
    "        diag_two_faces = []\n",
    "        diag_two_faces.extend(\n",
    "            [torch.diagonal(input_tensor[i, :, :]), torch.diagonal(input_tensor[:, i, :]), torch.diagonal(input_tensor[:, :, i]), \n",
    "            torch.diagonal(torch.fliplr(input_tensor)[i, :, :]), torch.diagonal(torch.fliplr(input_tensor)[:, i, :]), torch.diagonal(torch.fliplr(input_tensor)[:, :, i])] \n",
    "            for i in range(input_tensor.shape[0]))\n",
    "        diag_two_faces = [item for sublist in diag_two_faces for item in sublist]\n",
    "        \n",
    "        diag_three_faces = []\n",
    "        diag_three_faces = [[[[input_tensor[i, i, i], input_tensor[3 - i, i, i], input_tensor[i, 3 - i, i], input_tensor[i, i, 3 - i]] \n",
    "                            for i in range(4)][k][j] for j in range(4) for k in range(4)][l:l + 4] for l in range(0, 16, 4)]\n",
    "        diag_three_faces = [torch.tensor([t.item() for t in row]) for row in diag_three_faces]\n",
    "\n",
    "        horizontal_and_vertical_rows = []\n",
    "        horizontal_and_vertical_rows.extend([input_tensor[i, j, :], input_tensor[i, :, j], input_tensor[:, i, j]]\n",
    "                                            for i in range(input_tensor.shape[0]) for j in range(input_tensor.shape[0]))\n",
    "        horizontal_and_vertical_rows = [item for sublist in horizontal_and_vertical_rows for item in sublist]\n",
    "        \n",
    "        return horizontal_and_vertical_rows + diag_two_faces + diag_three_faces\n",
    "\n",
    "    def forward(self, x):\n",
    "        rows = self.get_rows(x)\n",
    "        return rows\n",
    "\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        self.structured_layer = StructuredLinear()\n",
    "        \n",
    "        self.custom_operation_layer = customDotProduct(input_size = 304, output_size = 304)\n",
    "\n",
    "        self.second_layer = nn.Linear(304, 32, bias = False)\n",
    "        init.xavier_normal_(self.second_layer.weight)\n",
    "\n",
    "        self.output_layer = nn.Linear(32, 1, bias = False)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.structured_layer(x)\n",
    "        x = self.custom_operation_layer(x)\n",
    "        x = self.act(x) # --> Tanh\n",
    "        x = self.second_layer(x)\n",
    "        x = self.act(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "# Example usage\n",
    "model= MyNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760d3665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "MyNeuralNetwork                          [4, 4, 4]                 [1, 1]                    --\n",
       "├─StructuredLinear: 1-1                  [4, 4, 4]                 [4]                       --\n",
       "├─customDotProduct: 1-2                  [4]                       [1, 304]                  1,216\n",
       "├─Tanh: 1-3                              [1, 304]                  [1, 304]                  --\n",
       "├─Linear: 1-4                            [1, 304]                  [1, 32]                   9,728\n",
       "├─Tanh: 1-5                              [1, 32]                   [1, 32]                   --\n",
       "├─Linear: 1-6                            [1, 32]                   [1, 1]                    32\n",
       "===================================================================================================================\n",
       "Total params: 10,976\n",
       "Trainable params: 10,976\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.01\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.04\n",
       "Estimated Total Size (MB): 0.04\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size = [4, 4, 4], col_names = ['input_size', 'output_size', 'num_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6fb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "EPSILON = 0.1\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b63194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(value_dict):\n",
    "    if np.random.random() > EPSILON:\n",
    "        return max(value_dict, key = lambda k: value_dict[k])\n",
    "    else:\n",
    "        return random.choice(list(value_dict.items()))[0]\n",
    "\n",
    "def func_modify(afterstate):\n",
    "    return torch.tensor([[[1 if cell == \"X\" else -1 if cell == \"O\" else 0 for cell in row] for row in layer] for layer in afterstate])\n",
    "\n",
    "def benchmark_policy_for_player2(action_list):\n",
    "    return np.random.choice(action_list)\n",
    "\n",
    "def train_td_model(model, num_episodes):\n",
    "    overall_loss = []\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        env = tictactoe_4x4.TicTacToe4x4x4()\n",
    "\n",
    "        terminated = False\n",
    "        current_state = torch.zeros((64))\n",
    "        reward = 0\n",
    "        player_turn = \"X\"\n",
    "        prev_afterstate = None\n",
    "        time_idx = 0\n",
    "        loss_list = []\n",
    "\n",
    "        while not terminated:\n",
    "            time_idx += 1\n",
    "            # Get actions space\n",
    "            action_space = env.get_action_space()\n",
    "            value_dict = {}\n",
    "            for action in action_space:\n",
    "                copy_tensor = current_state.detach().clone()\n",
    "                copy_tensor[action] = 1\n",
    "                value_dict[action] = model.forward(copy_tensor.view(4, 4, 4))\n",
    "            \n",
    "            # Here we choose action based on epsilon greedy\n",
    "            action = e_greedy(value_dict)\n",
    "            \n",
    "            current_state, reward, terminated, player_turn = env.step(action) # afterstate\n",
    "\n",
    "            if time_idx != 1:\n",
    "                v_new = reward + (GAMMA * model.forward(func_modify(current_state)))\n",
    "                v = model.forward(func_modify(prev_afterstate))\n",
    "                loss = loss_function(v, v_new)\n",
    "                loss_list.append(loss.item())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            prev_afterstate = current_state\n",
    "            # Player 2 -- use benchmark\n",
    "            player_2_action_space = env.get_action_space()\n",
    "            player_2_move = benchmark_policy_for_player2(player_2_action_space)\n",
    "            current_state, reward, terminated, player_turn = env.step(player_2_move)\n",
    "            \n",
    "            current_state = func_modify(current_state).view(64)\n",
    "\n",
    "        mean_loss = np.mean(loss_list)\n",
    "        print(f'Episode: {episode + 1}, Loss: {mean_loss}')\n",
    "        overall_loss.append(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ef1c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65e56383d8842819108c917b31fd1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Loss: 0.027332383097308503\n",
      "Episode: 2, Loss: 5.1807438116956526e-05\n",
      "Episode: 3, Loss: 0.00019809241865957681\n",
      "Episode: 4, Loss: 0.0003479837768656366\n",
      "Episode: 5, Loss: 0.025106080801919006\n",
      "Episode: 6, Loss: 0.01963167903727005\n",
      "Episode: 7, Loss: 0.021663222897347605\n",
      "Episode: 8, Loss: 0.0006128902910542978\n",
      "Episode: 9, Loss: 0.000210248991799705\n",
      "Episode: 10, Loss: 0.021815336566850786\n",
      "Episode: 11, Loss: 0.02595562695856451\n",
      "Episode: 12, Loss: 0.0006484965807445064\n",
      "Episode: 13, Loss: 0.002475650533122699\n",
      "Episode: 14, Loss: 7.743617253671194e-05\n",
      "Episode: 15, Loss: 0.0008059130394450711\n",
      "Episode: 16, Loss: 0.0001395504700754244\n",
      "Episode: 17, Loss: 0.028112671797819195\n",
      "Episode: 18, Loss: 0.0013305847987094938\n",
      "Episode: 19, Loss: 5.7880113849427696e-05\n",
      "Episode: 20, Loss: 0.00010876695018320207\n",
      "Episode: 21, Loss: 0.00022659868182017817\n",
      "Episode: 22, Loss: 0.00014484774108068117\n",
      "Episode: 23, Loss: 0.029176112286971628\n",
      "Episode: 24, Loss: 0.03018881896342825\n",
      "Episode: 25, Loss: 0.02953280595045366\n",
      "Episode: 26, Loss: 0.0005849175347975688\n",
      "Episode: 27, Loss: 0.036643410907485165\n",
      "Episode: 28, Loss: 0.0016485673304487136\n",
      "Episode: 29, Loss: 0.03101137074336293\n",
      "Episode: 30, Loss: 0.0014812947457314642\n",
      "Episode: 31, Loss: 0.0006921873757467741\n",
      "Episode: 32, Loss: 0.0014814473726750697\n",
      "Episode: 33, Loss: 0.00014924728032606924\n",
      "Episode: 34, Loss: 0.020414797540171097\n",
      "Episode: 35, Loss: 0.00016185491321607515\n",
      "Episode: 36, Loss: 0.021786963723846278\n",
      "Episode: 37, Loss: 0.0008730757313862662\n",
      "Episode: 38, Loss: 0.0004852914573802991\n",
      "Episode: 39, Loss: 0.0001675294177164774\n",
      "Episode: 40, Loss: 0.00014967914228236623\n",
      "Episode: 41, Loss: 9.99125359024166e-05\n",
      "Episode: 42, Loss: 0.00019434352826566383\n",
      "Episode: 43, Loss: 0.00023448534444665055\n",
      "Episode: 44, Loss: 0.00013008838023237704\n",
      "Episode: 45, Loss: 0.0001777571309759196\n",
      "Episode: 46, Loss: 6.23530620104545e-05\n",
      "Episode: 47, Loss: 8.225865888076913e-05\n",
      "Episode: 48, Loss: 5.459611364244665e-05\n",
      "Episode: 49, Loss: 3.339910199210513e-05\n",
      "Episode: 50, Loss: 3.716612904677656e-05\n",
      "Episode: 51, Loss: 0.00014802443686789957\n",
      "Episode: 52, Loss: 2.9782216174865386e-05\n",
      "Episode: 53, Loss: 0.01974645106358835\n",
      "Episode: 54, Loss: 0.02048516619821032\n",
      "Episode: 55, Loss: 0.0006056321040627413\n",
      "Episode: 56, Loss: 0.035725022646007486\n",
      "Episode: 57, Loss: 0.0008630341879324987\n",
      "Episode: 58, Loss: 0.00025057275110741884\n",
      "Episode: 59, Loss: 0.0008388266632328337\n",
      "Episode: 60, Loss: 0.00015491775310591847\n",
      "Episode: 61, Loss: 8.128250430535407e-05\n",
      "Episode: 62, Loss: 2.2380546335061998e-05\n",
      "Episode: 63, Loss: 0.03133232784920281\n",
      "Episode: 64, Loss: 0.00038506780115483253\n",
      "Episode: 65, Loss: 0.00016469934072792176\n",
      "Episode: 66, Loss: 0.00019946884323829295\n",
      "Episode: 67, Loss: 0.021196032812585603\n",
      "Episode: 68, Loss: 0.00026197630884578556\n",
      "Episode: 69, Loss: 0.00044338379923254803\n",
      "Episode: 70, Loss: 4.476181598618693e-05\n",
      "Episode: 71, Loss: 0.0008862699764904391\n",
      "Episode: 72, Loss: 0.03107668129391239\n",
      "Episode: 73, Loss: 0.0002486463762298996\n",
      "Episode: 74, Loss: 0.028655154859140747\n",
      "Episode: 75, Loss: 0.02637242298644398\n",
      "Episode: 76, Loss: 0.02437435728853942\n",
      "Episode: 77, Loss: 0.020441500532191032\n",
      "Episode: 78, Loss: 0.0005222540874456171\n",
      "Episode: 79, Loss: 0.02921624877181017\n",
      "Episode: 80, Loss: 0.03277709704551463\n",
      "Episode: 81, Loss: 0.022970201822599415\n",
      "Episode: 82, Loss: 0.000822208025783766\n",
      "Episode: 83, Loss: 0.0008901980237843483\n",
      "Episode: 84, Loss: 0.0004391929588354497\n",
      "Episode: 85, Loss: 0.028795924187387058\n",
      "Episode: 86, Loss: 0.0012512148506092753\n",
      "Episode: 87, Loss: 0.0005059800511211822\n",
      "Episode: 88, Loss: 0.02801308558050283\n",
      "Episode: 89, Loss: 7.028492030798361e-05\n",
      "Episode: 90, Loss: 0.00029463397605958074\n",
      "Episode: 91, Loss: 7.736833392115727e-05\n",
      "Episode: 92, Loss: 0.026693335344121164\n",
      "Episode: 93, Loss: 0.0009486755010110083\n",
      "Episode: 94, Loss: 0.00025187678642800567\n",
      "Episode: 95, Loss: 4.164693116640213e-05\n",
      "Episode: 96, Loss: 0.03185656342527555\n",
      "Episode: 97, Loss: 6.64424707273221e-05\n",
      "Episode: 98, Loss: 0.00026978964744728273\n",
      "Episode: 99, Loss: 0.022131821901645955\n",
      "Episode: 100, Loss: 0.022769253767455665\n",
      "Episode: 101, Loss: 0.0007043635976680724\n",
      "Episode: 102, Loss: 0.04957074243589248\n",
      "Episode: 103, Loss: 0.0012154137676892182\n",
      "Episode: 104, Loss: 0.02454623548335109\n",
      "Episode: 105, Loss: 0.028985699507604115\n",
      "Episode: 106, Loss: 0.027936070391774592\n",
      "Episode: 107, Loss: 0.06228914929243956\n",
      "Episode: 108, Loss: 0.04788654504874204\n",
      "Episode: 109, Loss: 0.04017266945580063\n",
      "Episode: 110, Loss: 0.02235087285417389\n",
      "Episode: 111, Loss: 0.03876187204415063\n",
      "Episode: 112, Loss: 0.038383051155092704\n",
      "Episode: 113, Loss: 0.003728799265725404\n",
      "Episode: 114, Loss: 0.03313756660925439\n",
      "Episode: 115, Loss: 0.0351398247457837\n",
      "Episode: 116, Loss: 0.0033008756122399063\n",
      "Episode: 117, Loss: 0.00020370420700671204\n",
      "Episode: 118, Loss: 0.034728157164090806\n",
      "Episode: 119, Loss: 0.032903480426674445\n",
      "Episode: 120, Loss: 0.04481668379432651\n",
      "Episode: 121, Loss: 0.025775765772678483\n",
      "Episode: 122, Loss: 0.032541303802304355\n",
      "Episode: 123, Loss: 0.038495983462780714\n",
      "Episode: 124, Loss: 0.03689648274788245\n",
      "Episode: 125, Loss: 0.0023849041523173278\n",
      "Episode: 126, Loss: 0.028749042064494763\n",
      "Episode: 127, Loss: 0.03358348285312856\n",
      "Episode: 128, Loss: 0.02192125596072266\n",
      "Episode: 129, Loss: 0.023052663620078608\n",
      "Episode: 130, Loss: 0.023949947778714887\n",
      "Episode: 131, Loss: 0.02734617912759063\n",
      "Episode: 132, Loss: 0.04000584025080074\n",
      "Episode: 133, Loss: 0.001186952918382863\n",
      "Episode: 134, Loss: 0.025489844106088736\n",
      "Episode: 135, Loss: 0.02278439340274677\n",
      "Episode: 136, Loss: 0.004311579682204562\n",
      "Episode: 137, Loss: 0.002204920843925922\n",
      "Episode: 138, Loss: 0.0007739554333538228\n",
      "Episode: 139, Loss: 0.03876848208065811\n",
      "Episode: 140, Loss: 0.026977624424837227\n",
      "Episode: 141, Loss: 0.020485635196110647\n",
      "Episode: 142, Loss: 0.06096840897827503\n",
      "Episode: 143, Loss: 0.022370168483099577\n",
      "Episode: 144, Loss: 0.03766212878558277\n",
      "Episode: 145, Loss: 0.0009819608932723207\n",
      "Episode: 146, Loss: 0.001665554929260045\n",
      "Episode: 147, Loss: 0.001038283009130772\n",
      "Episode: 148, Loss: 0.033760214213473846\n",
      "Episode: 149, Loss: 0.00103224433195237\n",
      "Episode: 150, Loss: 0.03691584138918684\n",
      "Episode: 151, Loss: 0.03795234712006845\n",
      "Episode: 152, Loss: 0.002338429734259585\n",
      "Episode: 153, Loss: 0.00010204378964737352\n",
      "Episode: 154, Loss: 0.04686112816780223\n",
      "Episode: 155, Loss: 0.0014778740491237841\n",
      "Episode: 156, Loss: 0.00038099339744235294\n",
      "Episode: 157, Loss: 0.00043779627586015835\n",
      "Episode: 158, Loss: 0.02704429469174774\n",
      "Episode: 159, Loss: 0.0011110751842130031\n",
      "Episode: 160, Loss: 0.0005943563316420225\n",
      "Episode: 161, Loss: 0.0007376668460210567\n",
      "Episode: 162, Loss: 0.00014398246783760746\n",
      "Episode: 163, Loss: 7.851035232005661e-05\n",
      "Episode: 164, Loss: 0.021334137983754567\n",
      "Episode: 165, Loss: 0.00018195261397598492\n",
      "Episode: 166, Loss: 0.02395641975669811\n",
      "Episode: 167, Loss: 0.001415821641909464\n",
      "Episode: 168, Loss: 0.00025158623582767275\n",
      "Episode: 169, Loss: 0.02268316228894078\n",
      "Episode: 170, Loss: 6.47770624155793e-05\n",
      "Episode: 171, Loss: 0.018597813708713426\n",
      "Episode: 172, Loss: 0.02917759710738513\n",
      "Episode: 173, Loss: 0.001031119320292159\n",
      "Episode: 174, Loss: 0.01939703709776159\n",
      "Episode: 175, Loss: 0.0007325507303903578\n",
      "Episode: 176, Loss: 0.001119094467933716\n",
      "Episode: 177, Loss: 0.029555914896385125\n",
      "Episode: 178, Loss: 0.00037178907495044334\n",
      "Episode: 179, Loss: 0.03282296026092505\n",
      "Episode: 180, Loss: 0.0006819220739920148\n",
      "Episode: 181, Loss: 0.0011549004757003074\n",
      "Episode: 182, Loss: 0.00014629487246035447\n",
      "Episode: 183, Loss: 0.028949730021951028\n",
      "Episode: 184, Loss: 0.0009254333802827579\n",
      "Episode: 185, Loss: 0.00013685500109911826\n",
      "Episode: 186, Loss: 0.00040765107034701487\n",
      "Episode: 187, Loss: 0.02329452310872625\n",
      "Episode: 188, Loss: 0.0001221398951561999\n",
      "Episode: 189, Loss: 0.0002871264745618999\n",
      "Episode: 190, Loss: 0.0001898142158549747\n",
      "Episode: 191, Loss: 0.00029767590918016227\n",
      "Episode: 192, Loss: 0.0003010939033742943\n",
      "Episode: 193, Loss: 0.032385936034431025\n",
      "Episode: 194, Loss: 0.00012406714988028652\n",
      "Episode: 195, Loss: 0.0002768419878975692\n",
      "Episode: 196, Loss: 0.024500656591906016\n",
      "Episode: 197, Loss: 0.0005437450713543512\n",
      "Episode: 198, Loss: 0.0002860240244444867\n",
      "Episode: 199, Loss: 0.0004890108290118933\n",
      "Episode: 200, Loss: 0.00014992746569727648\n",
      "Episode: 201, Loss: 0.0001235711004546098\n",
      "Episode: 202, Loss: 0.0006061448578239994\n",
      "Episode: 203, Loss: 0.00015546680481428492\n",
      "Episode: 204, Loss: 0.02264456706549661\n",
      "Episode: 205, Loss: 0.00034181126178928504\n",
      "Episode: 206, Loss: 0.024454372139734913\n",
      "Episode: 207, Loss: 0.02289847109441325\n",
      "Episode: 208, Loss: 0.025194748115035834\n",
      "Episode: 209, Loss: 0.00033779703279416634\n",
      "Episode: 210, Loss: 5.9590995421827875e-05\n",
      "Episode: 211, Loss: 0.0437424563520546\n",
      "Episode: 212, Loss: 0.02353514450746843\n",
      "Episode: 213, Loss: 0.0011122258706564025\n",
      "Episode: 214, Loss: 0.0002006210060160911\n",
      "Episode: 215, Loss: 4.017963556179893e-05\n",
      "Episode: 216, Loss: 0.0002811647708850532\n",
      "Episode: 217, Loss: 4.155330241929612e-05\n",
      "Episode: 218, Loss: 0.020715594626578127\n",
      "Episode: 219, Loss: 5.854339397046715e-05\n",
      "Episode: 220, Loss: 0.03111905420284226\n",
      "Episode: 221, Loss: 0.0007254785786017237\n",
      "Episode: 222, Loss: 0.033474588421800684\n",
      "Episode: 223, Loss: 0.017787685087905965\n",
      "Episode: 224, Loss: 0.0001585118875632817\n",
      "Episode: 225, Loss: 7.482247574941425e-05\n",
      "Episode: 226, Loss: 0.022588570014661292\n",
      "Episode: 227, Loss: 2.9344357366985954e-05\n",
      "Episode: 228, Loss: 0.001291904513808814\n",
      "Episode: 229, Loss: 0.0002976387357388636\n",
      "Episode: 230, Loss: 0.0013053356993921739\n",
      "Episode: 231, Loss: 5.6176437607515903e-05\n",
      "Episode: 232, Loss: 0.0002868121552975427\n",
      "Episode: 233, Loss: 0.0002791163413868552\n",
      "Episode: 234, Loss: 5.2851025564202494e-05\n",
      "Episode: 235, Loss: 0.0002792430999493588\n",
      "Episode: 236, Loss: 0.02432983217591982\n",
      "Episode: 237, Loss: 8.52498702605751e-05\n",
      "Episode: 238, Loss: 0.00037978306743918273\n",
      "Episode: 239, Loss: 0.023244136606081156\n",
      "Episode: 240, Loss: 0.00028324744353509513\n",
      "Episode: 241, Loss: 0.0004665717085747299\n",
      "Episode: 242, Loss: 0.0002791480496203543\n",
      "Episode: 243, Loss: 0.020515169778778014\n",
      "Episode: 244, Loss: 0.01791091571188493\n",
      "Episode: 245, Loss: 0.0229823540734034\n",
      "Episode: 246, Loss: 0.025166144550176364\n",
      "Episode: 247, Loss: 0.018136457881456427\n",
      "Episode: 248, Loss: 0.027343550018164988\n",
      "Episode: 249, Loss: 0.022045004057179626\n",
      "Episode: 250, Loss: 0.02858680284089843\n",
      "Episode: 251, Loss: 0.030663064674202663\n",
      "Episode: 252, Loss: 0.01769146309362231\n",
      "Episode: 253, Loss: 0.019592080975635327\n",
      "Episode: 254, Loss: 0.023714406325282954\n",
      "Episode: 255, Loss: 0.052348271732828126\n",
      "Episode: 256, Loss: 0.0013138043986576744\n",
      "Episode: 257, Loss: 0.0018139094988258036\n",
      "Episode: 258, Loss: 0.001597545217662074\n",
      "Episode: 259, Loss: 0.0001945305043591361\n",
      "Episode: 260, Loss: 0.0004293345651496887\n",
      "Episode: 261, Loss: 0.019384531579543014\n",
      "Episode: 262, Loss: 0.0003437354510464027\n",
      "Episode: 263, Loss: 0.0001424106983197109\n",
      "Episode: 264, Loss: 0.019223766830133278\n",
      "Episode: 265, Loss: 0.020856996779556262\n",
      "Episode: 266, Loss: 0.0002872050637531882\n",
      "Episode: 267, Loss: 0.00013769682057803342\n",
      "Episode: 268, Loss: 0.0003230302165667083\n",
      "Episode: 269, Loss: 0.0003070594024889446\n",
      "Episode: 270, Loss: 6.270291025675521e-05\n",
      "Episode: 271, Loss: 0.03234784508839691\n",
      "Episode: 272, Loss: 0.00011790176758993207\n",
      "Episode: 273, Loss: 0.030520004216555208\n",
      "Episode: 274, Loss: 0.00032779963930806844\n",
      "Episode: 275, Loss: 0.038870803648630194\n",
      "Episode: 276, Loss: 0.025436117024906037\n",
      "Episode: 277, Loss: 0.00039575829558950655\n",
      "Episode: 278, Loss: 0.022616789225124154\n",
      "Episode: 279, Loss: 0.00010431427788009589\n",
      "Episode: 280, Loss: 0.0248325516187929\n",
      "Episode: 281, Loss: 0.0001989840709939017\n",
      "Episode: 282, Loss: 0.02621295265685654\n",
      "Episode: 283, Loss: 0.028694803368125577\n",
      "Episode: 284, Loss: 0.03400867975750106\n",
      "Episode: 285, Loss: 0.029100794578369005\n",
      "Episode: 286, Loss: 0.02667029311252169\n",
      "Episode: 287, Loss: 0.027111794630079258\n",
      "Episode: 288, Loss: 0.027768602498968797\n",
      "Episode: 289, Loss: 0.0008361081325433805\n",
      "Episode: 290, Loss: 0.0007180661726606023\n",
      "Episode: 291, Loss: 0.00037959537950074364\n",
      "Episode: 292, Loss: 0.0006023343645210844\n",
      "Episode: 293, Loss: 0.00033600413704399604\n",
      "Episode: 294, Loss: 0.020203602197871835\n",
      "Episode: 295, Loss: 0.024975476946080658\n",
      "Episode: 296, Loss: 0.001027652988614136\n",
      "Episode: 297, Loss: 0.025503498029228036\n",
      "Episode: 298, Loss: 0.0011266264781846459\n",
      "Episode: 299, Loss: 0.0003048864838698743\n",
      "Episode: 300, Loss: 0.029456774297693635\n",
      "Episode: 301, Loss: 0.02257413841947536\n",
      "Episode: 302, Loss: 0.020991064728093534\n",
      "Episode: 303, Loss: 0.021987224749214614\n",
      "Episode: 304, Loss: 0.02425125446664149\n",
      "Episode: 305, Loss: 0.022505492210757438\n",
      "Episode: 306, Loss: 0.0004668618855836993\n",
      "Episode: 307, Loss: 0.0014311936666141871\n",
      "Episode: 308, Loss: 0.00023402583545717448\n",
      "Episode: 309, Loss: 0.00021661372257699194\n",
      "Episode: 310, Loss: 0.021091785970763038\n",
      "Episode: 311, Loss: 0.0003704178319742863\n",
      "Episode: 312, Loss: 0.025727082804063255\n",
      "Episode: 313, Loss: 0.028279728423171946\n",
      "Episode: 314, Loss: 0.02344674656699226\n",
      "Episode: 315, Loss: 6.638439144144057e-05\n",
      "Episode: 316, Loss: 0.00037836109986528755\n",
      "Episode: 317, Loss: 0.025501005423090344\n",
      "Episode: 318, Loss: 0.03745407372709527\n",
      "Episode: 319, Loss: 0.02318978956505864\n",
      "Episode: 320, Loss: 0.021295232360319633\n",
      "Episode: 321, Loss: 0.025702877838652525\n",
      "Episode: 322, Loss: 0.024560063412183\n",
      "Episode: 323, Loss: 0.0017156798477165132\n",
      "Episode: 324, Loss: 0.0018883257145493925\n",
      "Episode: 325, Loss: 0.0014689205082432683\n",
      "Episode: 326, Loss: 0.021594400710845157\n",
      "Episode: 327, Loss: 0.0006383002696869274\n",
      "Episode: 328, Loss: 0.001019878034658935\n",
      "Episode: 329, Loss: 0.0011894083281611912\n",
      "Episode: 330, Loss: 0.022471992419620317\n",
      "Episode: 331, Loss: 0.027710427022879337\n",
      "Episode: 332, Loss: 0.0001638233710187146\n",
      "Episode: 333, Loss: 0.026694980379035883\n",
      "Episode: 334, Loss: 0.00030646246947677963\n",
      "Episode: 335, Loss: 0.03017511236879121\n",
      "Episode: 336, Loss: 0.0356707433482436\n",
      "Episode: 337, Loss: 0.02591973802999645\n",
      "Episode: 338, Loss: 0.0277544835634842\n",
      "Episode: 339, Loss: 0.0011114066158813187\n",
      "Episode: 340, Loss: 0.032052126016830176\n",
      "Episode: 341, Loss: 0.022031866825446264\n",
      "Episode: 342, Loss: 0.024852306003908184\n",
      "Episode: 343, Loss: 0.0009727938544301651\n",
      "Episode: 344, Loss: 0.025816278501332005\n",
      "Episode: 345, Loss: 0.02142395613369903\n",
      "Episode: 346, Loss: 0.0016544602285559762\n",
      "Episode: 347, Loss: 0.01922303844351162\n",
      "Episode: 348, Loss: 0.02213704042589432\n",
      "Episode: 349, Loss: 0.01977382419805508\n",
      "Episode: 350, Loss: 0.02280193967822015\n",
      "Episode: 351, Loss: 0.019240525388410718\n",
      "Episode: 352, Loss: 0.0018111446355270834\n",
      "Episode: 353, Loss: 0.02866727704066066\n",
      "Episode: 354, Loss: 0.02627011706456816\n",
      "Episode: 355, Loss: 0.02749291601506343\n",
      "Episode: 356, Loss: 0.03244177767543685\n",
      "Episode: 357, Loss: 0.0011993454922242103\n",
      "Episode: 358, Loss: 0.00137691771177136\n",
      "Episode: 359, Loss: 0.02405527919634349\n",
      "Episode: 360, Loss: 0.021665246322153768\n",
      "Episode: 361, Loss: 0.0011972706050776033\n",
      "Episode: 362, Loss: 0.0010148160470028718\n",
      "Episode: 363, Loss: 0.0009865265169537452\n",
      "Episode: 364, Loss: 0.00027272025797628883\n",
      "Episode: 365, Loss: 0.0007028119745038046\n",
      "Episode: 366, Loss: 0.0010893146241869544\n",
      "Episode: 367, Loss: 0.025253090603008052\n",
      "Episode: 368, Loss: 0.035087212568984026\n",
      "Episode: 369, Loss: 0.024817325801932025\n",
      "Episode: 370, Loss: 0.0003105420914567623\n",
      "Episode: 371, Loss: 0.035163683633212574\n",
      "Episode: 372, Loss: 9.390815758555011e-05\n",
      "Episode: 373, Loss: 0.023392459734856173\n",
      "Episode: 374, Loss: 0.0006855334773187375\n",
      "Episode: 375, Loss: 0.029711785521552278\n",
      "Episode: 376, Loss: 0.051845929482308546\n",
      "Episode: 377, Loss: 0.03504366880527717\n",
      "Episode: 378, Loss: 0.0010037679314336856\n",
      "Episode: 379, Loss: 0.03767438335948596\n",
      "Episode: 380, Loss: 0.030026195951644956\n",
      "Episode: 381, Loss: 0.003626348033754969\n",
      "Episode: 382, Loss: 0.000773827797924501\n",
      "Episode: 383, Loss: 0.03197056883457857\n",
      "Episode: 384, Loss: 0.03622144953927394\n",
      "Episode: 385, Loss: 0.000790095859883877\n",
      "Episode: 386, Loss: 0.04384150179175271\n",
      "Episode: 387, Loss: 0.02406787120352681\n",
      "Episode: 388, Loss: 0.002831149213155077\n",
      "Episode: 389, Loss: 0.0005205027693926624\n",
      "Episode: 390, Loss: 0.03548058946650529\n",
      "Episode: 391, Loss: 0.03175816792645492\n",
      "Episode: 392, Loss: 0.0024501078840009592\n",
      "Episode: 393, Loss: 0.00030021174373697895\n",
      "Episode: 394, Loss: 0.0006712956031844747\n",
      "Episode: 395, Loss: 0.029509886019938373\n",
      "Episode: 396, Loss: 0.037140797744996235\n",
      "Episode: 397, Loss: 0.025406087723828987\n",
      "Episode: 398, Loss: 0.027481361702538218\n",
      "Episode: 399, Loss: 0.026791643948800972\n",
      "Episode: 400, Loss: 0.021831184900156585\n",
      "Episode: 401, Loss: 0.0352016567203726\n",
      "Episode: 402, Loss: 0.02333348895515548\n",
      "Episode: 403, Loss: 0.0259350194923008\n",
      "Episode: 404, Loss: 0.020050359777680726\n",
      "Episode: 405, Loss: 0.022700691963328898\n",
      "Episode: 406, Loss: 0.022504922487530023\n",
      "Episode: 407, Loss: 0.025044433307907878\n",
      "Episode: 408, Loss: 0.002487939076672774\n",
      "Episode: 409, Loss: 0.035133172330041325\n",
      "Episode: 410, Loss: 0.022093004616749568\n",
      "Episode: 411, Loss: 0.0005683086597230158\n",
      "Episode: 412, Loss: 0.0009654310257853164\n",
      "Episode: 413, Loss: 0.03809966845095173\n",
      "Episode: 414, Loss: 0.00032603500093692243\n",
      "Episode: 415, Loss: 0.03908165044443498\n",
      "Episode: 416, Loss: 0.0003058980336239377\n",
      "Episode: 417, Loss: 0.024952905330696683\n",
      "Episode: 418, Loss: 0.03402473382189223\n",
      "Episode: 419, Loss: 0.031861368300709056\n",
      "Episode: 420, Loss: 0.03597218154633689\n",
      "Episode: 421, Loss: 0.026302822157851874\n",
      "Episode: 422, Loss: 0.02426055241616041\n",
      "Episode: 423, Loss: 0.02662473918049102\n",
      "Episode: 424, Loss: 0.0252223906556992\n",
      "Episode: 425, Loss: 0.0004319170190931552\n",
      "Episode: 426, Loss: 0.0009606644400800438\n",
      "Episode: 427, Loss: 0.0241511803668907\n",
      "Episode: 428, Loss: 0.000373583320092271\n",
      "Episode: 429, Loss: 0.034486497883382934\n",
      "Episode: 430, Loss: 0.02886236831448817\n",
      "Episode: 431, Loss: 0.003664517868808042\n",
      "Episode: 432, Loss: 0.02940397141329803\n",
      "Episode: 433, Loss: 0.0001715073499697597\n",
      "Episode: 434, Loss: 0.024353933520998045\n",
      "Episode: 435, Loss: 0.024398138152671316\n",
      "Episode: 436, Loss: 0.03248614504571798\n",
      "Episode: 437, Loss: 0.031636735424011624\n",
      "Episode: 438, Loss: 0.004948469282453516\n",
      "Episode: 439, Loss: 0.03122163798729634\n",
      "Episode: 440, Loss: 0.00027237968206842805\n",
      "Episode: 441, Loss: 0.025750801155214397\n",
      "Episode: 442, Loss: 0.027538813105570624\n",
      "Episode: 443, Loss: 0.02464459798727902\n",
      "Episode: 444, Loss: 0.0035586996246517325\n",
      "Episode: 445, Loss: 0.0032874554469041564\n",
      "Episode: 446, Loss: 0.02737622509453086\n",
      "Episode: 447, Loss: 0.022830614745038877\n",
      "Episode: 448, Loss: 0.0023028622795877975\n",
      "Episode: 449, Loss: 0.02376979731125175\n",
      "Episode: 450, Loss: 0.023678803119865834\n",
      "Episode: 451, Loss: 0.00018480137126831463\n",
      "Episode: 452, Loss: 0.024915986802429617\n",
      "Episode: 453, Loss: 0.02290637807460658\n",
      "Episode: 454, Loss: 0.031293625854808244\n",
      "Episode: 455, Loss: 0.00012500090854257925\n",
      "Episode: 456, Loss: 0.024308817156308674\n",
      "Episode: 457, Loss: 0.024980178958952926\n",
      "Episode: 458, Loss: 0.0006350933306862316\n",
      "Episode: 459, Loss: 0.023347596948865214\n",
      "Episode: 460, Loss: 0.024932027686666518\n",
      "Episode: 461, Loss: 0.0003666031278536745\n",
      "Episode: 462, Loss: 0.00030857195059941965\n",
      "Episode: 463, Loss: 0.035224688183671575\n",
      "Episode: 464, Loss: 0.0314780491637066\n",
      "Episode: 465, Loss: 0.04763065081907472\n",
      "Episode: 466, Loss: 0.002400677826555789\n",
      "Episode: 467, Loss: 0.0015167113568687845\n",
      "Episode: 468, Loss: 0.04046182379090624\n",
      "Episode: 469, Loss: 0.03635783293429995\n",
      "Episode: 470, Loss: 0.00011586688977003077\n",
      "Episode: 471, Loss: 0.02822413362568265\n",
      "Episode: 472, Loss: 0.031053457333553882\n",
      "Episode: 473, Loss: 0.02514405595081227\n",
      "Episode: 474, Loss: 0.0008126954529993432\n",
      "Episode: 475, Loss: 0.03399057669836717\n",
      "Episode: 476, Loss: 0.02973176453378983\n",
      "Episode: 477, Loss: 0.031886641789848603\n",
      "Episode: 478, Loss: 0.029789339940309998\n",
      "Episode: 479, Loss: 0.024518338675414652\n",
      "Episode: 480, Loss: 0.0068836631020531055\n",
      "Episode: 481, Loss: 0.028782484411294718\n",
      "Episode: 482, Loss: 0.022513943241210654\n",
      "Episode: 483, Loss: 0.03336448498325458\n",
      "Episode: 484, Loss: 0.0017737529373047629\n",
      "Episode: 485, Loss: 0.04342436256896084\n",
      "Episode: 486, Loss: 0.033013418502650896\n",
      "Episode: 487, Loss: 0.0010199703368539651\n",
      "Episode: 488, Loss: 0.0008761718617790974\n",
      "Episode: 489, Loss: 0.0014790067781689231\n",
      "Episode: 490, Loss: 0.04637742733248658\n",
      "Episode: 491, Loss: 0.03247173015500136\n",
      "Episode: 492, Loss: 0.025839232803134025\n",
      "Episode: 493, Loss: 0.027976929663964256\n",
      "Episode: 494, Loss: 0.05363617626426276\n",
      "Episode: 495, Loss: 0.028929877470909533\n",
      "Episode: 496, Loss: 0.004439670075782942\n",
      "Episode: 497, Loss: 0.0033523961357483736\n",
      "Episode: 498, Loss: 0.023710217949966052\n",
      "Episode: 499, Loss: 0.02545071337763214\n",
      "Episode: 500, Loss: 0.037852118081566744\n",
      "Episode: 501, Loss: 0.0529033987065759\n",
      "Episode: 502, Loss: 0.04759636071715553\n",
      "Episode: 503, Loss: 0.004315253779168415\n",
      "Episode: 504, Loss: 0.0011758085419842246\n",
      "Episode: 505, Loss: 0.025005743802466895\n",
      "Episode: 506, Loss: 0.04355094134574529\n",
      "Episode: 507, Loss: 0.034391811085527024\n",
      "Episode: 508, Loss: 0.0419133480694048\n",
      "Episode: 509, Loss: 0.024622993806348716\n",
      "Episode: 510, Loss: 0.029306294392621828\n",
      "Episode: 511, Loss: 0.0458353102003457\n",
      "Episode: 512, Loss: 0.056858235936877985\n",
      "Episode: 513, Loss: 0.053321925797977184\n",
      "Episode: 514, Loss: 0.05054956158645447\n",
      "Episode: 515, Loss: 0.035542169468862995\n",
      "Episode: 516, Loss: 0.11816346223349683\n",
      "Episode: 517, Loss: 0.05894814796435336\n",
      "Episode: 518, Loss: 0.05570603639353067\n",
      "Episode: 519, Loss: 0.03726256022138033\n",
      "Episode: 520, Loss: 0.05140979299903847\n",
      "Episode: 521, Loss: 0.03740941643950969\n",
      "Episode: 522, Loss: 0.03979409619326665\n",
      "Episode: 523, Loss: 0.030421623002356766\n",
      "Episode: 524, Loss: 0.035307229651759066\n",
      "Episode: 525, Loss: 0.02934132349797768\n",
      "Episode: 526, Loss: 0.042107310109145146\n",
      "Episode: 527, Loss: 0.017412420057170447\n",
      "Episode: 528, Loss: 0.031472553820802\n",
      "Episode: 529, Loss: 0.06850492246448994\n",
      "Episode: 530, Loss: 0.09628611023072153\n",
      "Episode: 531, Loss: 0.0465273077522094\n",
      "Episode: 532, Loss: 0.028868608149030642\n",
      "Episode: 533, Loss: 0.030538381587274444\n",
      "Episode: 534, Loss: 0.024637770481852138\n",
      "Episode: 535, Loss: 0.03386239583051065\n",
      "Episode: 536, Loss: 0.0028588488324073425\n",
      "Episode: 537, Loss: 0.031709049218786985\n",
      "Episode: 538, Loss: 0.050016558344941585\n",
      "Episode: 539, Loss: 0.03601229937135031\n",
      "Episode: 540, Loss: 0.02569454518452403\n",
      "Episode: 541, Loss: 0.006888549253812925\n",
      "Episode: 542, Loss: 0.038264085296153404\n",
      "Episode: 543, Loss: 0.042713279094338454\n",
      "Episode: 544, Loss: 0.02455118133965537\n",
      "Episode: 545, Loss: 0.03752549221849222\n",
      "Episode: 546, Loss: 0.025347110100217275\n",
      "Episode: 547, Loss: 0.026133971786245973\n",
      "Episode: 548, Loss: 0.005809999405755661\n",
      "Episode: 549, Loss: 0.05992238246835768\n",
      "Episode: 550, Loss: 0.04911236650365985\n",
      "Episode: 551, Loss: 0.03179376500338549\n",
      "Episode: 552, Loss: 0.018810487126756925\n",
      "Episode: 553, Loss: 0.06659298032172956\n",
      "Episode: 554, Loss: 0.03558068571232177\n",
      "Episode: 555, Loss: 0.039172367528798144\n",
      "Episode: 556, Loss: 0.06009456772590056\n",
      "Episode: 557, Loss: 0.0504076440265635\n",
      "Episode: 558, Loss: 0.026994644932600703\n",
      "Episode: 559, Loss: 0.037300248847300696\n",
      "Episode: 560, Loss: 0.06239629473670253\n",
      "Episode: 561, Loss: 0.05170136895030737\n",
      "Episode: 562, Loss: 0.054732752794128224\n",
      "Episode: 563, Loss: 0.05052220832779616\n",
      "Episode: 564, Loss: 0.06408539243178286\n",
      "Episode: 565, Loss: 0.05078918196765395\n",
      "Episode: 566, Loss: 0.0641791608184576\n",
      "Episode: 567, Loss: 0.05614435635410094\n",
      "Episode: 568, Loss: 0.05114847218750843\n",
      "Episode: 569, Loss: 0.05644154559704475\n",
      "Episode: 570, Loss: 0.06545148782788601\n",
      "Episode: 571, Loss: 0.05084680395278459\n",
      "Episode: 572, Loss: 0.025966711069582073\n",
      "Episode: 573, Loss: 0.054011268385996423\n",
      "Episode: 574, Loss: 0.02560950802713209\n",
      "Episode: 575, Loss: 0.05956129857804626\n",
      "Episode: 576, Loss: 0.0666388395256945\n",
      "Episode: 577, Loss: 0.00035125669993855507\n",
      "Episode: 578, Loss: 0.051478349161334334\n",
      "Episode: 579, Loss: 0.030943837967546035\n",
      "Episode: 580, Loss: 0.05862390917855009\n",
      "Episode: 581, Loss: 0.03266937275111559\n",
      "Episode: 582, Loss: 0.042874070582305045\n",
      "Episode: 583, Loss: 0.059388539039900934\n",
      "Episode: 584, Loss: 0.02785623216137041\n",
      "Episode: 585, Loss: 0.08964628167450428\n",
      "Episode: 586, Loss: 0.06449948911904357\n",
      "Episode: 587, Loss: 0.07991271633848858\n",
      "Episode: 588, Loss: 0.051835171040147544\n",
      "Episode: 589, Loss: 0.06627785277669318\n",
      "Episode: 590, Loss: 0.07994657362966488\n",
      "Episode: 591, Loss: 0.06549689066741848\n",
      "Episode: 592, Loss: 0.05588851743959822\n",
      "Episode: 593, Loss: 0.037105820673917024\n",
      "Episode: 594, Loss: 0.0569671664852649\n",
      "Episode: 595, Loss: 0.051759371906518935\n",
      "Episode: 596, Loss: 0.05475979116454255\n",
      "Episode: 597, Loss: 0.08815141022205353\n",
      "Episode: 598, Loss: 0.009955521028202314\n",
      "Episode: 599, Loss: 0.08640212767447035\n",
      "Episode: 600, Loss: 0.08786905168866117\n",
      "Episode: 601, Loss: 0.0557320832934541\n",
      "Episode: 602, Loss: 0.06339330418850295\n",
      "Episode: 603, Loss: 0.08422010221208136\n",
      "Episode: 604, Loss: 0.039314873158505896\n",
      "Episode: 605, Loss: 0.07280452050534743\n",
      "Episode: 606, Loss: 0.048343729603892334\n",
      "Episode: 607, Loss: 0.05762047180905938\n",
      "Episode: 608, Loss: 0.058415569456701635\n",
      "Episode: 609, Loss: 0.05730795827339108\n",
      "Episode: 610, Loss: 0.05636331700770825\n",
      "Episode: 611, Loss: 0.051269620739185486\n",
      "Episode: 612, Loss: 0.05112248822115362\n",
      "Episode: 613, Loss: 0.054665431091052596\n",
      "Episode: 614, Loss: 0.03823044902570213\n",
      "Episode: 615, Loss: 0.06381801434326917\n",
      "Episode: 616, Loss: 0.04523360030725598\n",
      "Episode: 617, Loss: 0.05667461503617233\n",
      "Episode: 618, Loss: 0.040769551803047456\n",
      "Episode: 619, Loss: 0.0039639218981998665\n",
      "Episode: 620, Loss: 0.03494630398798857\n",
      "Episode: 621, Loss: 0.0034733587700420307\n",
      "Episode: 622, Loss: 0.08756044141288537\n",
      "Episode: 623, Loss: 0.046717986557632686\n",
      "Episode: 624, Loss: 0.02925810606533332\n",
      "Episode: 625, Loss: 0.03387310154212173\n",
      "Episode: 626, Loss: 0.0376990205498741\n",
      "Episode: 627, Loss: 0.043615111404506024\n",
      "Episode: 628, Loss: 0.02633060374983356\n",
      "Episode: 629, Loss: 0.03221254147190068\n",
      "Episode: 630, Loss: 0.06437695439672098\n",
      "Episode: 631, Loss: 0.0037235870724543927\n",
      "Episode: 632, Loss: 0.041999342278965436\n",
      "Episode: 633, Loss: 0.0424079995136708\n",
      "Episode: 634, Loss: 0.029156222990942688\n",
      "Episode: 635, Loss: 0.07433614888092659\n",
      "Episode: 636, Loss: 0.0017797650052671088\n",
      "Episode: 637, Loss: 0.05034496681764722\n",
      "Episode: 638, Loss: 0.032708480391496174\n",
      "Episode: 639, Loss: 0.026473344054206142\n",
      "Episode: 640, Loss: 0.03172599439858459\n",
      "Episode: 641, Loss: 0.027045832708245142\n",
      "Episode: 642, Loss: 0.01820716046333675\n",
      "Episode: 643, Loss: 0.024650860935960535\n",
      "Episode: 644, Loss: 0.00219363531113441\n",
      "Episode: 645, Loss: 0.03804500892551409\n",
      "Episode: 646, Loss: 0.06915413627284579\n",
      "Episode: 647, Loss: 0.052672842711520694\n",
      "Episode: 648, Loss: 0.022341726159719532\n",
      "Episode: 649, Loss: 0.008732286595234957\n",
      "Episode: 650, Loss: 0.0028066232375224967\n",
      "Episode: 651, Loss: 0.03362075011578521\n",
      "Episode: 652, Loss: 0.007262502063225072\n",
      "Episode: 653, Loss: 0.03199051814117411\n",
      "Episode: 654, Loss: 0.02507452273130184\n",
      "Episode: 655, Loss: 0.021607402126061392\n",
      "Episode: 656, Loss: 0.002552551399276126\n",
      "Episode: 657, Loss: 0.02715862808771716\n",
      "Episode: 658, Loss: 0.008112054338441036\n",
      "Episode: 659, Loss: 0.02637599578668993\n",
      "Episode: 660, Loss: 0.0019273517676197116\n",
      "Episode: 661, Loss: 0.02413802002556622\n",
      "Episode: 662, Loss: 0.02660014845666107\n",
      "Episode: 663, Loss: 0.023682352127767103\n",
      "Episode: 664, Loss: 0.02370635806344268\n",
      "Episode: 665, Loss: 0.021598752026027165\n",
      "Episode: 666, Loss: 0.030291823521890495\n",
      "Episode: 667, Loss: 0.006249108200771995\n",
      "Episode: 668, Loss: 0.02270944197850137\n",
      "Episode: 669, Loss: 0.002782137793097015\n",
      "Episode: 670, Loss: 0.02706046458423281\n",
      "Episode: 671, Loss: 0.01990801894183581\n",
      "Episode: 672, Loss: 0.016774825697228022\n",
      "Episode: 673, Loss: 0.024147634237703063\n",
      "Episode: 674, Loss: 0.021504080727765995\n",
      "Episode: 675, Loss: 0.0014152695781604052\n",
      "Episode: 676, Loss: 0.002038249133552957\n",
      "Episode: 677, Loss: 0.02477154875362574\n",
      "Episode: 678, Loss: 0.029680516296398958\n",
      "Episode: 679, Loss: 0.030071326800680254\n",
      "Episode: 680, Loss: 0.03459331220559155\n",
      "Episode: 681, Loss: 0.02516119700123909\n",
      "Episode: 682, Loss: 0.02424374727427046\n",
      "Episode: 683, Loss: 0.019018754793002947\n",
      "Episode: 684, Loss: 0.0004445477162724793\n",
      "Episode: 685, Loss: 0.02911846205824986\n",
      "Episode: 686, Loss: 0.0010304916227647078\n",
      "Episode: 687, Loss: 0.02167747695663517\n",
      "Episode: 688, Loss: 0.02045136671495129\n",
      "Episode: 689, Loss: 0.02599401350432475\n",
      "Episode: 690, Loss: 0.02719805633075469\n",
      "Episode: 691, Loss: 0.0232669084407064\n",
      "Episode: 692, Loss: 0.0011603935742771551\n",
      "Episode: 693, Loss: 0.03084459659547302\n",
      "Episode: 694, Loss: 0.032433010247586935\n",
      "Episode: 695, Loss: 0.059676721692085266\n",
      "Episode: 696, Loss: 0.024513734380317793\n",
      "Episode: 697, Loss: 0.019832713320325143\n",
      "Episode: 698, Loss: 0.0038871966282588094\n",
      "Episode: 699, Loss: 0.001066260508870507\n",
      "Episode: 700, Loss: 0.021938175351533574\n",
      "Episode: 701, Loss: 0.020488063392197848\n",
      "Episode: 702, Loss: 0.029575681749913128\n",
      "Episode: 703, Loss: 0.002437360588677014\n",
      "Episode: 704, Loss: 0.024005354119953363\n",
      "Episode: 705, Loss: 0.020889736008899963\n",
      "Episode: 706, Loss: 0.025054398532016752\n",
      "Episode: 707, Loss: 0.0007369008699656247\n",
      "Episode: 708, Loss: 0.001792486909297705\n",
      "Episode: 709, Loss: 0.024294805581865312\n",
      "Episode: 710, Loss: 0.001329693131226936\n",
      "Episode: 711, Loss: 0.000700829783223288\n",
      "Episode: 712, Loss: 0.023788175525100524\n",
      "Episode: 713, Loss: 0.0024099234565669162\n",
      "Episode: 714, Loss: 0.03602630197132212\n",
      "Episode: 715, Loss: 0.01964264974910848\n",
      "Episode: 716, Loss: 0.018917215783199028\n",
      "Episode: 717, Loss: 0.0012948998397551545\n",
      "Episode: 718, Loss: 0.026593684374434122\n",
      "Episode: 719, Loss: 0.0018157483838063644\n",
      "Episode: 720, Loss: 0.017639393703000356\n",
      "Episode: 721, Loss: 0.028628308375256577\n",
      "Episode: 722, Loss: 0.03548393832007995\n",
      "Episode: 723, Loss: 0.0019347308975414177\n",
      "Episode: 724, Loss: 0.052444530789418624\n",
      "Episode: 725, Loss: 0.020247023356855787\n",
      "Episode: 726, Loss: 0.02889199596859271\n",
      "Episode: 727, Loss: 0.023296459927924815\n",
      "Episode: 728, Loss: 0.03132609866510475\n",
      "Episode: 729, Loss: 0.025256215579594253\n",
      "Episode: 730, Loss: 0.0016507182408531662\n",
      "Episode: 731, Loss: 0.0002090246105339172\n",
      "Episode: 732, Loss: 0.04104075289036369\n",
      "Episode: 733, Loss: 0.03404847005943742\n",
      "Episode: 734, Loss: 0.02441316631728796\n",
      "Episode: 735, Loss: 0.0007534599188147694\n",
      "Episode: 736, Loss: 0.0017743253210937837\n",
      "Episode: 737, Loss: 0.02477983228222911\n",
      "Episode: 738, Loss: 0.002186056155291486\n",
      "Episode: 739, Loss: 0.00045780780055793\n",
      "Episode: 740, Loss: 0.0008513899040671902\n",
      "Episode: 741, Loss: 0.000420683825116915\n",
      "Episode: 742, Loss: 0.000752329116127252\n",
      "Episode: 743, Loss: 0.0006196458113360374\n",
      "Episode: 744, Loss: 0.02227331961264347\n",
      "Episode: 745, Loss: 0.021756290686122764\n",
      "Episode: 746, Loss: 0.0009344303455172565\n",
      "Episode: 747, Loss: 0.03458217021789548\n",
      "Episode: 748, Loss: 0.03670033019284347\n",
      "Episode: 749, Loss: 0.0016618304580333643\n",
      "Episode: 750, Loss: 0.01714961259302286\n",
      "Episode: 751, Loss: 0.013922333651360455\n",
      "Episode: 752, Loss: 0.00011864487805723911\n",
      "Episode: 753, Loss: 0.0017978657202687687\n",
      "Episode: 754, Loss: 0.016405520117980642\n",
      "Episode: 755, Loss: 0.04651194782672974\n",
      "Episode: 756, Loss: 0.030985142050613992\n",
      "Episode: 757, Loss: 0.001157548228626673\n",
      "Episode: 758, Loss: 0.022436159037124526\n",
      "Episode: 759, Loss: 0.0023496643164738393\n",
      "Episode: 760, Loss: 0.00037771335438960437\n",
      "Episode: 761, Loss: 0.023986039317346046\n",
      "Episode: 762, Loss: 0.000265666816024653\n",
      "Episode: 763, Loss: 0.02508203843454895\n",
      "Episode: 764, Loss: 0.033615517821000855\n",
      "Episode: 765, Loss: 0.0017200211541421595\n",
      "Episode: 766, Loss: 0.11102753444722566\n",
      "Episode: 767, Loss: 0.0015555108689397912\n",
      "Episode: 768, Loss: 0.03420876950569133\n",
      "Episode: 769, Loss: 0.004013581550680101\n",
      "Episode: 770, Loss: 0.0005105504180467809\n",
      "Episode: 771, Loss: 0.024339271733241654\n",
      "Episode: 772, Loss: 0.02111682590248165\n",
      "Episode: 773, Loss: 0.0014965515015508875\n",
      "Episode: 774, Loss: 0.048122416449414375\n",
      "Episode: 775, Loss: 0.0009558992771251889\n",
      "Episode: 776, Loss: 0.02761684934333008\n",
      "Episode: 777, Loss: 0.0005686506476616374\n",
      "Episode: 778, Loss: 0.018991005822420488\n",
      "Episode: 779, Loss: 0.0009720722064311563\n",
      "Episode: 780, Loss: 0.00016762163716066425\n",
      "Episode: 781, Loss: 0.03494453109418113\n",
      "Episode: 782, Loss: 0.0009140634991088427\n",
      "Episode: 783, Loss: 0.03478822761373733\n",
      "Episode: 784, Loss: 0.06776894504894569\n",
      "Episode: 785, Loss: 0.028128338414449793\n",
      "Episode: 786, Loss: 0.001080631059461926\n",
      "Episode: 787, Loss: 0.0008828786340018269\n",
      "Episode: 788, Loss: 0.029184537377408802\n",
      "Episode: 789, Loss: 0.022178315961649663\n",
      "Episode: 790, Loss: 0.0027878770893689114\n",
      "Episode: 791, Loss: 0.04425651244508282\n",
      "Episode: 792, Loss: 0.0006617015142789266\n",
      "Episode: 793, Loss: 0.0004330473265701371\n",
      "Episode: 794, Loss: 0.021145716860622128\n",
      "Episode: 795, Loss: 0.03977250229842282\n",
      "Episode: 796, Loss: 0.03005914867021277\n",
      "Episode: 797, Loss: 0.0010267188075279894\n",
      "Episode: 798, Loss: 0.03041932337241098\n",
      "Episode: 799, Loss: 0.06578273353807162\n",
      "Episode: 800, Loss: 0.07076746093836543\n",
      "Episode: 801, Loss: 0.03218538587264318\n",
      "Episode: 802, Loss: 0.003574200582410082\n",
      "Episode: 803, Loss: 0.0025024535219320874\n",
      "Episode: 804, Loss: 0.0008375094505306781\n",
      "Episode: 805, Loss: 0.04206579508578064\n",
      "Episode: 806, Loss: 0.022226016565972322\n",
      "Episode: 807, Loss: 0.0003901132300737887\n",
      "Episode: 808, Loss: 0.02095481924757081\n",
      "Episode: 809, Loss: 0.0167884369364757\n",
      "Episode: 810, Loss: 0.0009460484281869588\n",
      "Episode: 811, Loss: 0.00025079234962634345\n",
      "Episode: 812, Loss: 0.0003480958702686407\n",
      "Episode: 813, Loss: 0.03102567616050627\n",
      "Episode: 814, Loss: 0.0016968815398286097\n",
      "Episode: 815, Loss: 0.030799438885317887\n",
      "Episode: 816, Loss: 0.021733085684610905\n",
      "Episode: 817, Loss: 0.02162208658338434\n",
      "Episode: 818, Loss: 0.02643555566919531\n",
      "Episode: 819, Loss: 0.028004996181039132\n",
      "Episode: 820, Loss: 0.0011646777033762958\n",
      "Episode: 821, Loss: 0.0008026943037341047\n",
      "Episode: 822, Loss: 0.0002495835120100764\n",
      "Episode: 823, Loss: 0.0001344135716863093\n",
      "Episode: 824, Loss: 0.02402467266281085\n",
      "Episode: 825, Loss: 0.021245871558754084\n",
      "Episode: 826, Loss: 0.019963306147870424\n",
      "Episode: 827, Loss: 0.028415835442842763\n",
      "Episode: 828, Loss: 0.020082329311872645\n",
      "Episode: 829, Loss: 0.020543863089490817\n",
      "Episode: 830, Loss: 0.031141926985863114\n",
      "Episode: 831, Loss: 0.01953771059246113\n",
      "Episode: 832, Loss: 0.03329699773085951\n",
      "Episode: 833, Loss: 0.021565573966654483\n",
      "Episode: 834, Loss: 0.04801569979613305\n",
      "Episode: 835, Loss: 0.0060342315316827605\n",
      "Episode: 836, Loss: 0.00019268794198978867\n",
      "Episode: 837, Loss: 0.03283668213593542\n",
      "Episode: 838, Loss: 0.0030494234982446264\n",
      "Episode: 839, Loss: 0.00507882421152317\n",
      "Episode: 840, Loss: 0.0012949976770691453\n",
      "Episode: 841, Loss: 0.004011406679758996\n",
      "Episode: 842, Loss: 0.023716735796420432\n",
      "Episode: 843, Loss: 0.01906139191662928\n",
      "Episode: 844, Loss: 0.0010560890873688657\n",
      "Episode: 845, Loss: 0.029616674007115762\n",
      "Episode: 846, Loss: 0.0019808115748674027\n",
      "Episode: 847, Loss: 0.030463023529261012\n",
      "Episode: 848, Loss: 0.019430605492804412\n",
      "Episode: 849, Loss: 5.5168296086094415e-05\n",
      "Episode: 850, Loss: 0.02009744834675568\n",
      "Episode: 851, Loss: 0.028238169906090762\n",
      "Episode: 852, Loss: 0.0018750566897714244\n",
      "Episode: 853, Loss: 7.871983811291727e-05\n",
      "Episode: 854, Loss: 0.017982750188026458\n",
      "Episode: 855, Loss: 0.027384456223747595\n",
      "Episode: 856, Loss: 0.012049604783533141\n",
      "Episode: 857, Loss: 0.0013686583380106078\n",
      "Episode: 858, Loss: 0.0314534962227223\n",
      "Episode: 859, Loss: 0.021477052831703043\n",
      "Episode: 860, Loss: 0.045861260034143925\n",
      "Episode: 861, Loss: 0.02750106609617766\n",
      "Episode: 862, Loss: 0.04578520808718167\n",
      "Episode: 863, Loss: 0.030881745849219062\n",
      "Episode: 864, Loss: 0.0326719800081524\n",
      "Episode: 865, Loss: 0.04600411484534561\n",
      "Episode: 866, Loss: 0.02956576914687048\n",
      "Episode: 867, Loss: 0.03279826257029425\n",
      "Episode: 868, Loss: 0.0309597929667234\n",
      "Episode: 869, Loss: 0.022923864669185907\n",
      "Episode: 870, Loss: 0.030206186458331907\n",
      "Episode: 871, Loss: 0.03149611159991158\n",
      "Episode: 872, Loss: 0.02449469536077231\n",
      "Episode: 873, Loss: 0.027849151137551025\n",
      "Episode: 874, Loss: 0.022135506849735975\n",
      "Episode: 875, Loss: 0.024919388639628438\n",
      "Episode: 876, Loss: 0.02656433728083761\n",
      "Episode: 877, Loss: 0.02804181673855055\n",
      "Episode: 878, Loss: 0.029952248282955094\n",
      "Episode: 879, Loss: 0.035239625327868944\n",
      "Episode: 880, Loss: 0.012131711374285814\n",
      "Episode: 881, Loss: 0.0021549323060098325\n",
      "Episode: 882, Loss: 0.0008461536266869037\n",
      "Episode: 883, Loss: 0.0451821660171845\n",
      "Episode: 884, Loss: 0.024988734799470485\n",
      "Episode: 885, Loss: 0.01553449452330824\n",
      "Episode: 886, Loss: 0.017470012728468785\n",
      "Episode: 887, Loss: 0.021387552432133815\n",
      "Episode: 888, Loss: 0.021066089039052827\n",
      "Episode: 889, Loss: 0.01980922356597148\n",
      "Episode: 890, Loss: 0.023817093098292236\n",
      "Episode: 891, Loss: 0.01915840802747779\n",
      "Episode: 892, Loss: 0.03554260777891614\n",
      "Episode: 893, Loss: 0.006326261374462254\n",
      "Episode: 894, Loss: 0.009380091560295418\n",
      "Episode: 895, Loss: 0.007461066384980529\n",
      "Episode: 896, Loss: 0.020102592834778728\n",
      "Episode: 897, Loss: 0.018832014553481713\n",
      "Episode: 898, Loss: 0.02261489383979804\n",
      "Episode: 899, Loss: 0.0006964339642763662\n",
      "Episode: 900, Loss: 0.013429796631113631\n",
      "Episode: 901, Loss: 0.020912340422000852\n",
      "Episode: 902, Loss: 0.025145008435780863\n",
      "Episode: 903, Loss: 0.01237190995480988\n",
      "Episode: 904, Loss: 0.008721354254521429\n",
      "Episode: 905, Loss: 0.007796032125725105\n",
      "Episode: 906, Loss: 0.009140075540472026\n",
      "Episode: 907, Loss: 0.01236078012152575\n",
      "Episode: 908, Loss: 0.003419363714911583\n",
      "Episode: 909, Loss: 0.03766760899441124\n",
      "Episode: 910, Loss: 0.02131430254303268\n",
      "Episode: 911, Loss: 0.011624224516312153\n",
      "Episode: 912, Loss: 0.04516935558876867\n",
      "Episode: 913, Loss: 0.00017055100468294567\n",
      "Episode: 914, Loss: 0.011102910843975324\n",
      "Episode: 915, Loss: 0.020441427284031703\n",
      "Episode: 916, Loss: 0.031380247407450955\n",
      "Episode: 917, Loss: 0.0019489369072592428\n",
      "Episode: 918, Loss: 0.0643572052654885\n",
      "Episode: 919, Loss: 0.035261688934718614\n",
      "Episode: 920, Loss: 0.027692619362243914\n",
      "Episode: 921, Loss: 0.004899799361010082\n",
      "Episode: 922, Loss: 0.025027366996710043\n",
      "Episode: 923, Loss: 0.03827948541147634\n",
      "Episode: 924, Loss: 0.022822285530981475\n",
      "Episode: 925, Loss: 0.02166436604034061\n",
      "Episode: 926, Loss: 0.02435788773891545\n",
      "Episode: 927, Loss: 0.0026946995353682723\n",
      "Episode: 928, Loss: 0.03187212216435\n",
      "Episode: 929, Loss: 0.00023217774040731724\n",
      "Episode: 930, Loss: 0.0002631130780995965\n",
      "Episode: 931, Loss: 0.026854002019389454\n",
      "Episode: 932, Loss: 0.023843872108045616\n",
      "Episode: 933, Loss: 0.013911332046308626\n",
      "Episode: 934, Loss: 0.02609944125870243\n",
      "Episode: 935, Loss: 0.0037038539885543287\n",
      "Episode: 936, Loss: 0.02411759819369763\n",
      "Episode: 937, Loss: 0.009176212095413715\n",
      "Episode: 938, Loss: 0.024629311107188794\n",
      "Episode: 939, Loss: 0.015028651565808104\n",
      "Episode: 940, Loss: 0.01544422106235288\n",
      "Episode: 941, Loss: 0.05591931915841997\n",
      "Episode: 942, Loss: 0.007560772828155142\n",
      "Episode: 943, Loss: 0.010353663649584632\n",
      "Episode: 944, Loss: 0.048073954283609055\n",
      "Episode: 945, Loss: 0.040488760266453025\n",
      "Episode: 946, Loss: 0.036881071230579564\n",
      "Episode: 947, Loss: 0.043988225655084534\n",
      "Episode: 948, Loss: 0.040566112718977365\n",
      "Episode: 949, Loss: 0.03261011761302749\n",
      "Episode: 950, Loss: 0.03530719067202881\n",
      "Episode: 951, Loss: 0.02987544722855091\n",
      "Episode: 952, Loss: 0.04516742859656612\n",
      "Episode: 953, Loss: 0.03999410088484486\n",
      "Episode: 954, Loss: 0.033623278799215464\n",
      "Episode: 955, Loss: 0.03428118986388048\n",
      "Episode: 956, Loss: 0.01816195024366607\n",
      "Episode: 957, Loss: 0.02592593936446974\n",
      "Episode: 958, Loss: 0.02460755966603756\n",
      "Episode: 959, Loss: 0.02383602294139564\n",
      "Episode: 960, Loss: 0.01940732116054278\n",
      "Episode: 961, Loss: 0.021287281648255885\n",
      "Episode: 962, Loss: 0.004976398316406024\n",
      "Episode: 963, Loss: 0.025281702539359685\n",
      "Episode: 964, Loss: 0.026204742025583982\n",
      "Episode: 965, Loss: 0.024450393859297037\n",
      "Episode: 966, Loss: 0.022190351150735903\n",
      "Episode: 967, Loss: 0.026745492849601617\n",
      "Episode: 968, Loss: 0.028134975474159774\n",
      "Episode: 969, Loss: 0.021124553595048685\n",
      "Episode: 970, Loss: 0.024775582074653357\n",
      "Episode: 971, Loss: 0.0016798920021756202\n",
      "Episode: 972, Loss: 0.015626991807948798\n",
      "Episode: 973, Loss: 0.024377021750506874\n",
      "Episode: 974, Loss: 0.03022985311690718\n",
      "Episode: 975, Loss: 0.016364187494173166\n",
      "Episode: 976, Loss: 0.02518014312829564\n",
      "Episode: 977, Loss: 0.0012960172044921862\n",
      "Episode: 978, Loss: 0.0005136447183427994\n",
      "Episode: 979, Loss: 0.03015255613718182\n",
      "Episode: 980, Loss: 0.016806296939557798\n",
      "Episode: 981, Loss: 0.01999856335066852\n",
      "Episode: 982, Loss: 0.04117833700001938\n",
      "Episode: 983, Loss: 0.0022946843495057173\n",
      "Episode: 984, Loss: 0.0024919678898320066\n",
      "Episode: 985, Loss: 0.029200781078543514\n",
      "Episode: 986, Loss: 0.017472542929075036\n",
      "Episode: 987, Loss: 0.0023921700521896128\n",
      "Episode: 988, Loss: 0.020295558100193033\n",
      "Episode: 989, Loss: 0.002184028308233773\n",
      "Episode: 990, Loss: 0.002383941947482526\n",
      "Episode: 991, Loss: 0.012090699339751154\n",
      "Episode: 992, Loss: 0.03689360773475008\n",
      "Episode: 993, Loss: 0.019489879038883374\n",
      "Episode: 994, Loss: 0.018958249886054546\n",
      "Episode: 995, Loss: 0.015804550275788642\n",
      "Episode: 996, Loss: 0.014952492131851614\n",
      "Episode: 997, Loss: 0.01764554081064489\n",
      "Episode: 998, Loss: 0.01890614860894857\n",
      "Episode: 999, Loss: 0.014108984973669672\n",
      "Episode: 1000, Loss: 0.021978721022605896\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_episodes = 1000  # Number of episodes for training\n",
    "loss = train_td_model(model, num_episodes)  # Train the model\n",
    "\n",
    "# Save the trained model\n",
    "# os.makedirs('C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/reinforcement_learning/Project/Phase_3_3D_Tic_Tac_Toe/models', exist_ok = True)\n",
    "# model_path = 'C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/reinforcement_learning/Project/Phase_3_3D_Tic_Tac_Toe/models/td_tictactoe_model.pth'\n",
    "# save_model(model, model_path)\n",
    "\n",
    "# model_path\n",
    "\n",
    "# 1) Turn all of this to device compatible - (cuda) (train for 1000 episodes and store weights every 100)\n",
    "# 2) Once the model has been trained, make a function which recieves the models weights and the Neural Network Object and the current action space and state\n",
    "# the function takes the weights, and inserts them into the neural network object and computes forward pass for each of the possible transitions and returns\n",
    "# the action corresponding to the maximum value. Be careful as the state that is being passed is a list of lists of lists. So to make that compatible to the\n",
    "# forward pass, use the func_modify function to get it into a 3D tensor form. Also remember to convert each of the actions (in integer form) to coardinates\n",
    "# form using the get_coardinates function. Then transition using those coardinates. Once you have now computed the maximum action value, convert it back\n",
    "# to integer using get_position and that will be returned\n",
    "\n",
    "# Inference Loop - u will play ur policy (gotten from above function) with another baseline policy like random for the other player. Count the number of\n",
    "# ones/ minus ones/zeros and report % win, % loss, % draw.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77b53112",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = model.state_dict()\n",
    "torch.save(model_weights, 'C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/reinforcement_learning/Project/Phase_3_3D_Tic_Tac_Toe/models/run_1_1000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16e66e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(position):\n",
    "    x = int((position % 16) % 4)\n",
    "    y = int((position % 16) / 4)\n",
    "    z = int(position / 16)\n",
    "\n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79d4e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_player1(state_dict, model, action_space, observation):\n",
    "    model.load_state_dict(state_dict)\n",
    "    value_dict = {}\n",
    "    for action in action_space:\n",
    "        x, y, z = get_coordinates(action)\n",
    "        copy_tensor = func_modify(observation).detach().clone()\n",
    "        copy_tensor[x, y, z] = 1\n",
    "        value_dict[action] = model.forward(copy_tensor.view(4, 4, 4))\n",
    "    action = max(value_dict, key = lambda k: value_dict[k])\n",
    "    return action\n",
    "\n",
    "def policy_player2(action_space):\n",
    "    return benchmark_policy_for_player2(action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9e83be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_game(policy_player1, policy_player2, state_dict, model, N = 100000, render_mode = \"computer\"):\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        env = tictactoe_4x4.TicTacToe4x4x4(render_mode)\n",
    "\n",
    "        terminated = 0\n",
    "        observation = [[[\" \" for _ in range(4)] for _ in range(4)] for _ in range(4)]\n",
    "        reward = 0\n",
    "        player_turn = \"X\"\n",
    "        while not terminated:\n",
    "            action_space = env.get_action_space()\n",
    "\n",
    "            if player_turn == \"X\":\n",
    "                action = policy_player1(state_dict, model, action_space, observation)\n",
    "            else:\n",
    "                action = policy_player2(action_space)\n",
    "\n",
    "            observation, reward, terminated, player_turn = env.step(action)\n",
    "        if reward == 1: wins += 1\n",
    "        if reward == -1: losses += 1\n",
    "        if reward == 0: draws += 1\n",
    "\n",
    "    print(f'win percentage: {(wins / N) * 100}%')\n",
    "    print(f'lose percentage: {(losses / N) * 100}%')\n",
    "    print(f'draw percentage: {(draws / N) * 100}%')\n",
    "\n",
    "new_model = MyNeuralNetwork()\n",
    "state_load = torch.load('C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/reinforcement_learning/Project/Phase_3_3D_Tic_Tac_Toe/models/run_1_1000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b02d0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_one_game(policy_player1, policy_player2, state_load, new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386dc07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
