{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e9351a0",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning for 3D Tic Tac Toe\n",
    "\n",
    "This notebook contains the implementation of a Temporal Difference (TD) learning model using a Deep Q-Network (DQN) for playing 3D 4x4x4 Tic Tac Toe. The implementation is based on the approach outlined in the provided paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261768de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Setting Directory\n",
    "os.chdir('C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/reinforcement_learning/Project/')\n",
    "\n",
    "from python_scripts import state_formulation, utils, algorithm, tictactoe_4x4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "import torch.nn.init as init\n",
    "from tqdm.autonotebook import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a36f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDotProduct(nn.Module):\n",
    "    def __init__(self, input_size, output_size, block_size = 4):\n",
    "        super(customDotProduct, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.block_size = block_size\n",
    "        # Convert structure_weight to nn.Parameter\n",
    "        self.structure_weight = torch.zeros((self.output_size, self.input_size))\n",
    "        self.structure_weight = self.get_block_weights(self.structure_weight, block_size)\n",
    "        self.structure_weight = nn.ParameterList([nn.Parameter(sw.float()) for sw in self.structure_weight])\n",
    "\n",
    "    def get_block_weights(self, weight_list, block_size):\n",
    "        for i in range(0, 304, block_size):\n",
    "            weight_list[i: i + block_size, i: i + block_size] = init.xavier_normal_(torch.randn(block_size, block_size))\n",
    "        learnable_blocks = [weight_list[i:i + block_size, i:i + block_size] for i in range(0, weight_list.shape[0], block_size)]\n",
    "        updated = [block for block in learnable_blocks]\n",
    "        return updated\n",
    "    \n",
    "    def forward(self, feature_map):\n",
    "        self.feature_map = [fm.float() for fm in feature_map]\n",
    "        # Calculate dot products and concatenate along dim = 1\n",
    "        concatenated_products = torch.cat([torch.matmul(fm.unsqueeze(0), sw) for fm, sw in zip(self.feature_map, self.structure_weight)], dim = 1)\n",
    "        return concatenated_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09ad6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StructuredLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StructuredLinear, self).__init__()\n",
    "\n",
    "    def get_rows(self, input_tensor):\n",
    "        # Get diagonals (across 2 faces),digonals (across 3 faces) and horizontal and vertical rows\n",
    "        diag_two_faces = []\n",
    "        diag_two_faces.extend(\n",
    "            [torch.diagonal(input_tensor[i, :, :]), torch.diagonal(input_tensor[:, i, :]), torch.diagonal(input_tensor[:, :, i]), \n",
    "            torch.diagonal(torch.fliplr(input_tensor)[i, :, :]), torch.diagonal(torch.fliplr(input_tensor)[:, i, :]), torch.diagonal(torch.fliplr(input_tensor)[:, :, i])] \n",
    "            for i in range(input_tensor.shape[0]))\n",
    "        diag_two_faces = [item for sublist in diag_two_faces for item in sublist]\n",
    "        \n",
    "        diag_three_faces = []\n",
    "        diag_three_faces = [[[[input_tensor[i, i, i], input_tensor[3 - i, i, i], input_tensor[i, 3 - i, i], input_tensor[i, i, 3 - i]] \n",
    "                            for i in range(4)][k][j] for j in range(4) for k in range(4)][l:l + 4] for l in range(0, 16, 4)]\n",
    "        diag_three_faces = [torch.tensor([t.item() for t in row]) for row in diag_three_faces]\n",
    "\n",
    "        horizontal_and_vertical_rows = []\n",
    "        horizontal_and_vertical_rows.extend([input_tensor[i, j, :], input_tensor[i, :, j], input_tensor[:, i, j]]\n",
    "                                            for i in range(input_tensor.shape[0]) for j in range(input_tensor.shape[0]))\n",
    "        horizontal_and_vertical_rows = [item for sublist in horizontal_and_vertical_rows for item in sublist]\n",
    "        \n",
    "        return horizontal_and_vertical_rows + diag_two_faces + diag_three_faces\n",
    "\n",
    "    def forward(self, x):\n",
    "        rows = self.get_rows(x)\n",
    "        return rows\n",
    "\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        self.structured_layer = StructuredLinear()\n",
    "        \n",
    "        self.custom_operation_layer = customDotProduct(input_size = 304, output_size = 304)\n",
    "\n",
    "        self.second_layer = nn.Linear(304, 32, bias = False)\n",
    "        init.xavier_normal_(self.second_layer.weight)\n",
    "\n",
    "        self.output_layer = nn.Linear(32, 1, bias = False)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.structured_layer(x)\n",
    "        x = self.custom_operation_layer(x)\n",
    "        x = self.act(x) # --> Tanh\n",
    "        x = self.second_layer(x)\n",
    "        x = self.act(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "# Example usage\n",
    "# If using a pre-trained model (see report) load the weights like\n",
    "model = MyNeuralNetwork()\n",
    "state_dict_runs = torch.load('C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/reinforcement_learning/Project/Phase_3_3D_Tic_Tac_Toe/models/ep_6600.pth', map_location = torch.device(device))\n",
    "model.load_state_dict(state_dict_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760d3665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "MyNeuralNetwork                          [4, 4, 4]                 [1, 1]                    --\n",
       "├─StructuredLinear: 1-1                  [4, 4, 4]                 [4]                       --\n",
       "├─customDotProduct: 1-2                  [4]                       [1, 304]                  1,216\n",
       "├─Tanh: 1-3                              [1, 304]                  [1, 304]                  --\n",
       "├─Linear: 1-4                            [1, 304]                  [1, 32]                   9,728\n",
       "├─Tanh: 1-5                              [1, 32]                   [1, 32]                   --\n",
       "├─Linear: 1-6                            [1, 32]                   [1, 1]                    32\n",
       "===================================================================================================================\n",
       "Total params: 10,976\n",
       "Trainable params: 10,976\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.01\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.04\n",
       "Estimated Total Size (MB): 0.04\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size = [4, 4, 4], col_names = ['input_size', 'output_size', 'num_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6fb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "EPSILON = 0.1\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b63194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(value_dict):\n",
    "    if np.random.random() > EPSILON:\n",
    "        return max(value_dict, key = lambda k: value_dict[k])\n",
    "    else:\n",
    "        return random.choice(list(value_dict.items()))[0]\n",
    "\n",
    "def func_modify(afterstate): # 3d list to 1d tensor\n",
    "    return torch.tensor([[[1 if cell == \"X\" else -1 if cell == \"O\" else 0 for cell in row] for row in layer] for layer in afterstate])\n",
    "\n",
    "# Initlly agent trained by random actions of other player\n",
    "def benchmark_policy_for_player2(action_list):\n",
    "    return np.random.choice(action_list)\n",
    "\n",
    "def get_coordinates(position):\n",
    "    x = int((position % 16) % 4)\n",
    "    y = int((position % 16) / 4)\n",
    "    z = int(position / 16)\n",
    "\n",
    "    return x, y, z\n",
    "\n",
    "# After 5000 or so episodes we resorted to the model itself for the other player's moves. In that case use the \n",
    "# defined model weights for it\n",
    "# For player 2 policy - change ur directory for 5200 here\n",
    "# model2 = MyNeuralNetwork()\n",
    "# state_load = torch.load('C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/reinforcement_learning/Project/Phase_3_3D_Tic_Tac_Toe/models/ep_5000.pth', map_location = torch.device(device))\n",
    "# model2.load_state_dict(state_load)\n",
    "\n",
    "def policy_player2(model, action_space, observation):\n",
    "    value_dict = {}\n",
    "    for action in action_space:\n",
    "        x, y, z = get_coordinates(action)\n",
    "        copy_tensor = func_modify(observation).detach().clone()\n",
    "        copy_tensor[x, y, z] = -1\n",
    "        value_dict[action] = model.forward(copy_tensor.view(4, 4, 4))\n",
    "    action = max(value_dict, key = lambda k: value_dict[k])\n",
    "    return action\n",
    "\n",
    "def train_td_model(model, num_episodes):\n",
    "    model.train()\n",
    "    overall_loss = []\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        env = tictactoe_4x4.TicTacToe4x4x4()\n",
    "\n",
    "        terminated = False\n",
    "        current_state = torch.zeros((64), device = device)\n",
    "        reward = 0\n",
    "        player_turn = \"X\"\n",
    "        prev_afterstate = None\n",
    "        time_idx = 0\n",
    "        loss_list = []\n",
    "\n",
    "        while not terminated:\n",
    "            time_idx += 1\n",
    "            # Get actions space\n",
    "            action_space = env.get_action_space()\n",
    "            value_dict = {}\n",
    "            for action in action_space:\n",
    "                copy_tensor = current_state.detach().clone()\n",
    "                copy_tensor[action] = 1\n",
    "                value_dict[action] = model.forward(copy_tensor.view(4, 4, 4))\n",
    "\n",
    "            # Here we choose action based on epsilon greedy\n",
    "            action = e_greedy(value_dict)\n",
    "\n",
    "            current_state, reward, terminated, player_turn = env.step(action) # afterstate\n",
    "\n",
    "            if time_idx != 1:\n",
    "                v_new = reward + (GAMMA * model.forward(func_modify(current_state).to(device)))\n",
    "                v = model.forward(func_modify(prev_afterstate).to(device))\n",
    "                loss = loss_function(v, v_new)\n",
    "                loss_list.append(loss.item())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            prev_afterstate = current_state\n",
    "            # Player 2 -- use random or model itself\n",
    "            player_2_action_space = env.get_action_space()\n",
    "            player_2_move = benchmark_policy_for_player2(player_2_action_space) ## --> random\n",
    "            # player_2_move = policy_player2(model2, action_space, current_state) # --> model itself\n",
    "            current_state, reward, terminated, player_turn = env.step(player_2_move)\n",
    "\n",
    "            current_state = func_modify(current_state).view(64).to(device)\n",
    "\n",
    "        mean_loss = np.mean(loss_list)\n",
    "        # print(f'Episode: {episode + 1}, Loss: {mean_loss}')\n",
    "        overall_loss.append(mean_loss)\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f'episode: {episode + 1}, loss: {mean_loss}')\n",
    "            model_path = f'C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/reinforcement_learning/Project/Phase_3_3D_Tic_Tac_Toe/models/ep_{episode + 1 + 6600}.pth'\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38ef1c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20683bd39c448d2b4d20938dd3ca1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Number of episodes for training\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_td_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 80\u001b[0m, in \u001b[0;36mtrain_td_model\u001b[1;34m(model, num_episodes)\u001b[0m\n\u001b[0;32m     78\u001b[0m player_2_action_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_action_space()\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# player_2_move = benchmark_policy_for_player2(player_2_action_space) ## --> random\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m player_2_move \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_player2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# --> model itself\u001b[39;00m\n\u001b[0;32m     81\u001b[0m current_state, reward, terminated, player_turn \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(player_2_move)\n\u001b[0;32m     83\u001b[0m current_state \u001b[38;5;241m=\u001b[39m func_modify(current_state)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m, in \u001b[0;36mpolicy_player2\u001b[1;34m(model, action_space, observation)\u001b[0m\n\u001b[0;32m     32\u001b[0m     copy_tensor \u001b[38;5;241m=\u001b[39m func_modify(observation)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m     33\u001b[0m     copy_tensor[x, y, z] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 34\u001b[0m     value_dict[action] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(value_dict, key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m k: value_dict[k])\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "Cell \u001b[1;32mIn[3], line 46\u001b[0m, in \u001b[0;36mMyNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructured_layer(x)\n\u001b[0;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_operation_layer(x)\n\u001b[1;32m---> 46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# --> Tanh\u001b[39;00m\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecond_layer(x)\n\u001b[0;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:359\u001b[0m, in \u001b[0;36mTanh.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_episodes = 1000  # Number of episodes for training\n",
    "loss = train_td_model(model, num_episodes)  # Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c386dc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c6e34a09d1431abe0c4b84f9d05776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win percentage: 43.0%\n",
      "lose percentage: 56.99999999999999%\n",
      "draw percentage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Inference/Testing Against random policy\n",
    "# Inference\n",
    "def get_coordinates(position):\n",
    "    x = int((position % 16) % 4)\n",
    "    y = int((position % 16) / 4)\n",
    "    z = int(position / 16)\n",
    "\n",
    "    return x, y, z\n",
    "def policy_player1(model, action_space, observation):\n",
    "    value_dict = {}\n",
    "    for action in action_space:\n",
    "        x, y, z = get_coordinates(action)\n",
    "        copy_tensor = func_modify(observation).detach().clone()\n",
    "        copy_tensor[x, y, z] = 1\n",
    "        value_dict[action] = model.forward(copy_tensor.view(4, 4, 4))\n",
    "    action = max(value_dict, key = lambda k: value_dict[k])\n",
    "    return action\n",
    "\n",
    "def play_games(policy_player1, policy_player2, model, N = 100, render_mode = \"computer\"):\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "\n",
    "    for i in tqdm(range(N)):\n",
    "        env = tictactoe_4x4.TicTacToe4x4x4(render_mode)\n",
    "\n",
    "        terminated = 0\n",
    "        observation = [[[\" \" for _ in range(4)] for _ in range(4)] for _ in range(4)]\n",
    "        reward = 0\n",
    "        player_turn = \"X\"\n",
    "        while not terminated:\n",
    "            action_space = env.get_action_space()\n",
    "\n",
    "            if player_turn == \"X\":\n",
    "                action = policy_player1(model, action_space, observation)\n",
    "            else:\n",
    "                action = benchmark_policy_for_player2(action_space)\n",
    "\n",
    "            observation, reward, terminated, player_turn = env.step(action)\n",
    "        if reward == 1: wins += 1\n",
    "        if reward == -1: losses += 1\n",
    "        if reward == 0: draws += 1\n",
    "\n",
    "    print(f'win percentage: {(wins / N) * 100}%')\n",
    "    print(f'lose percentage: {(losses / N) * 100}%')\n",
    "    print(f'draw percentage: {(draws / N) * 100}%')\n",
    "\n",
    "new_model = MyNeuralNetwork()\n",
    "state_new = torch.load('C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/reinforcement_learning/Project/Phase_3_3D_Tic_Tac_Toe/models/ep_8300.pth', map_location = torch.device(device))\n",
    "new_model.load_state_dict(state_new)\n",
    "play_games(policy_player1, benchmark_policy_for_player2, new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4690c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
